{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Glass-Box Transformer: A Fully Interpretable Language Model\n",
        "Built with explicit interpretability at every layer\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class InterpretableAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention mechanism with explicit interpretability.\n",
        "    Each head has a semantic purpose that we can inspect.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, n_heads: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        # Separate projections for interpretability\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.o_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Head names for interpretability\n",
        "        self.head_names = [f\"Head_{i}\" for i in range(n_heads)]\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask=None) -> Tuple[torch.Tensor, Dict]:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Project and split into heads\n",
        "        Q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.o_proj(attn_output)\n",
        "\n",
        "        # Store interpretability info\n",
        "        interpretability = {\n",
        "            'attention_weights': attn_weights.detach(),\n",
        "            'head_names': self.head_names\n",
        "        }\n",
        "\n",
        "        return output, interpretability\n",
        "\n",
        "\n",
        "class InterpretableFFN(nn.Module):\n",
        "    \"\"\"\n",
        "    Feed-forward network with explicit feature interpretability.\n",
        "    We can see which neurons activate for which patterns.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, d_ff: int):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Dict]:\n",
        "        # First layer with activation\n",
        "        hidden = self.activation(self.linear1(x))\n",
        "        output = self.linear2(hidden)\n",
        "\n",
        "        # Store interpretability info\n",
        "        interpretability = {\n",
        "            'hidden_activations': hidden.detach(),\n",
        "            'neuron_importance': hidden.abs().mean(dim=(0, 1)).detach()\n",
        "        }\n",
        "\n",
        "        return output, interpretability\n",
        "\n",
        "\n",
        "class GlassBoxTransformerLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single transformer layer with full interpretability tracking.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attention = InterpretableAttention(d_model, n_heads)\n",
        "        self.ffn = InterpretableFFN(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask=None) -> Tuple[torch.Tensor, Dict]:\n",
        "        # Attention block with residual\n",
        "        attn_out, attn_interp = self.attention(self.norm1(x), mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        # FFN block with residual\n",
        "        ffn_out, ffn_interp = self.ffn(self.norm2(x))\n",
        "        x = x + self.dropout(ffn_out)\n",
        "\n",
        "        # Combine interpretability info\n",
        "        interpretability = {\n",
        "            'attention': attn_interp,\n",
        "            'ffn': ffn_interp,\n",
        "            'residual_contribution': {\n",
        "                'attention': attn_out.abs().mean().item(),\n",
        "                'ffn': ffn_out.abs().mean().item()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return x, interpretability\n",
        "\n",
        "\n",
        "class ContextAwareGlassBoxTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 128,\n",
        "        n_layers: int = 4,\n",
        "        n_heads: int = 4,\n",
        "        d_ff: int = 512,\n",
        "        max_seq_len: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        tokenizer=None,  # CRITICAL: Must pass tokenizer\n",
        "        use_context_features: bool = True  # Toggle context features\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.use_context_features = use_context_features\n",
        "\n",
        "        # Contextual embeddings instead of static\n",
        "        if use_context_features:\n",
        "            self.token_embedding = ContextualEmbedding(vocab_size, d_model, context_window=3)\n",
        "\n",
        "            # Semantic role layer (only if tokenizer provided)\n",
        "            if tokenizer is not None:\n",
        "                self.semantic_layer = SemanticRoleAttention(d_model, vocab_size)\n",
        "            else:\n",
        "                self.semantic_layer = None\n",
        "                print(\"‚ö†Ô∏è  Warning: No tokenizer provided, semantic roles disabled\")\n",
        "\n",
        "            # Adaptive rule layer\n",
        "            self.rule_layer = AdaptiveRuleLayer(d_model, max_rules=100)\n",
        "        else:\n",
        "            # Fallback to standard embedding\n",
        "            self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "            self.semantic_layer = None\n",
        "            self.rule_layer = None\n",
        "\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        # Regular transformer layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            GlassBoxTransformerLayer(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.output_norm = nn.LayerNorm(d_model)\n",
        "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask=None) -> Tuple[torch.Tensor, Dict]:\n",
        "        batch_size, seq_len = x.shape\n",
        "\n",
        "        # Contextual embeddings\n",
        "        if self.use_context_features and isinstance(self.token_embedding, ContextualEmbedding):\n",
        "            token_emb, emb_interp = self.token_embedding(x)\n",
        "        else:\n",
        "            token_emb = self.token_embedding(x)\n",
        "            emb_interp = {}\n",
        "\n",
        "        # Position embeddings\n",
        "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
        "        pos_emb = self.position_embedding(positions)\n",
        "\n",
        "        hidden = self.dropout(token_emb + pos_emb)\n",
        "\n",
        "        # Apply semantic role analysis (if available)\n",
        "        semantic_interp = {}\n",
        "        if self.semantic_layer is not None and self.tokenizer is not None:\n",
        "            hidden, semantic_interp = self.semantic_layer(hidden, x, self.tokenizer)\n",
        "\n",
        "        # Apply adaptive rules (if available)\n",
        "        rule_interp = {}\n",
        "        if self.rule_layer is not None:\n",
        "            hidden, rule_interp = self.rule_layer(hidden)\n",
        "\n",
        "        # Regular transformer layers\n",
        "        layer_interpretability = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            hidden, interp = layer(hidden, mask)\n",
        "            interp['layer_idx'] = i\n",
        "            layer_interpretability.append(interp)\n",
        "\n",
        "        # Output\n",
        "        hidden = self.output_norm(hidden)\n",
        "        logits = self.output_proj(hidden)\n",
        "\n",
        "        # Complete interpretability package\n",
        "        full_interpretability = {\n",
        "            'contextual_embeddings': emb_interp,\n",
        "            'semantic_roles': semantic_interp,\n",
        "            'rules': rule_interp,\n",
        "            'layers': layer_interpretability,\n",
        "            'final_hidden': hidden.detach()\n",
        "        }\n",
        "\n",
        "        return logits, full_interpretability\n",
        "\n",
        "class GlassBoxVisualizer:\n",
        "    \"\"\"\n",
        "    Visualization tools for understanding the glass-box transformer.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def plot_attention_pattern(attn_weights: np.ndarray, tokens: List[str] = None):\n",
        "        \"\"\"Plot attention patterns for all heads.\"\"\"\n",
        "        n_heads = attn_weights.shape[0]\n",
        "        fig, axes = plt.subplots(1, n_heads, figsize=(4*n_heads, 4))\n",
        "\n",
        "        if n_heads == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for head_idx, ax in enumerate(axes):\n",
        "            sns.heatmap(attn_weights[head_idx], ax=ax, cmap='YlOrRd',\n",
        "                       xticklabels=tokens, yticklabels=tokens)\n",
        "            ax.set_title(f'Head {head_idx}')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_layer_contributions(interpretability: Dict):\n",
        "        \"\"\"Plot how much each layer contributes.\"\"\"\n",
        "        layers = []\n",
        "        attn_contrib = []\n",
        "        ffn_contrib = []\n",
        "\n",
        "        for layer_info in interpretability['layers']:\n",
        "            layers.append(layer_info['layer_idx'])\n",
        "            contrib = layer_info['residual_contribution']\n",
        "            attn_contrib.append(contrib['attention'])\n",
        "            ffn_contrib.append(contrib['ffn'])\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        x = np.arange(len(layers))\n",
        "        width = 0.35\n",
        "\n",
        "        ax.bar(x - width/2, attn_contrib, width, label='Attention')\n",
        "        ax.bar(x + width/2, ffn_contrib, width, label='FFN')\n",
        "\n",
        "        ax.set_xlabel('Layer')\n",
        "        ax.set_ylabel('Average Contribution')\n",
        "        ax.set_title('Layer-wise Contributions to Output')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels([f'Layer {i}' for i in layers])\n",
        "        ax.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 1: Test the Model Architecture\n",
        "# =============================================================================\n",
        "def test_architecture():\n",
        "    print(\"üîç Glass-Box Transformer Initialized\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create a small model\n",
        "    vocab_size = 1000\n",
        "    model = GlassBoxTransformer(\n",
        "        vocab_size=vocab_size,\n",
        "        d_model=128,\n",
        "        n_layers=4,\n",
        "        n_heads=4,\n",
        "        d_ff=512,\n",
        "        max_seq_len=128\n",
        "    )\n",
        "\n",
        "    # Count parameters\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Model Parameters: {n_params:,}\")\n",
        "    print(f\"Model Size: ~{n_params * 4 / 1024 / 1024:.2f} MB (FP32)\")\n",
        "\n",
        "    # Test forward pass\n",
        "    batch_size = 2\n",
        "    seq_len = 10\n",
        "    dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "\n",
        "    print(f\"\\nTest Input Shape: {dummy_input.shape}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, interpretability = model(dummy_input)\n",
        "\n",
        "    print(f\"Output Shape: {logits.shape}\")\n",
        "    print(f\"\\nInterpretability Package Contains:\")\n",
        "    print(f\"  - {len(interpretability['layers'])} layer explanations\")\n",
        "    print(f\"  - Token embeddings: {interpretability['token_embeddings'].shape}\")\n",
        "    print(f\"  - Position embeddings: {interpretability['position_embeddings'].shape}\")\n",
        "    print(f\"  - Final hidden states: {interpretability['final_hidden'].shape}\")\n",
        "\n",
        "    print(\"\\n‚úÖ Glass-Box Transformer Ready!\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "X37k7TXlbUd7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 2: COMPREHENSIVE DOMAIN-SPECIFIC TOKENIZER\n",
        "# =============================================================================\n",
        "class ComprehensiveChurnTokenizer:\n",
        "    \"\"\"\n",
        "    Enterprise-grade tokenizer with domain-specific vocabulary.\n",
        "    Organized by linguistic and semantic categories for interpretability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # =============================================================================\n",
        "        # SENTIMENT & EMOTION WORDS\n",
        "        # =============================================================================\n",
        "\n",
        "        # Strong positive sentiment\n",
        "        strong_positive = [\n",
        "            'amazing', 'awesome', 'excellent', 'outstanding', 'exceptional', 'phenomenal',\n",
        "            'spectacular', 'superb', 'wonderful', 'fantastic', 'brilliant', 'magnificent',\n",
        "            'marvelous', 'fabulous', 'terrific', 'stellar', 'supreme', 'unbeatable',\n",
        "            'extraordinary', 'remarkable', 'impressive', 'stunning', 'dazzling'\n",
        "        ]\n",
        "\n",
        "        # Moderate positive sentiment\n",
        "        moderate_positive = [\n",
        "            'good', 'great', 'nice', 'fine', 'pleasant', 'positive', 'satisfactory',\n",
        "            'acceptable', 'decent', 'solid', 'adequate', 'reasonable', 'fair',\n",
        "            'delightful', 'enjoyable', 'lovely', 'sweet', 'pretty', 'favorable'\n",
        "        ]\n",
        "\n",
        "        # Weak positive sentiment\n",
        "        weak_positive = [\n",
        "            'okay', 'ok', 'alright', 'passable', 'tolerable', 'bearable', 'manageable'\n",
        "        ]\n",
        "\n",
        "        # Strong negative sentiment\n",
        "        strong_negative = [\n",
        "            'terrible', 'horrible', 'awful', 'atrocious', 'abysmal', 'dreadful',\n",
        "            'appalling', 'horrendous', 'deplorable', 'disastrous', 'catastrophic',\n",
        "            'nightmarish', 'unbearable', 'intolerable', 'unacceptable', 'abominable',\n",
        "            'pathetic', 'miserable', 'wretched', 'despicable', 'detestable'\n",
        "        ]\n",
        "\n",
        "        # Moderate negative sentiment\n",
        "        moderate_negative = [\n",
        "            'bad', 'poor', 'subpar', 'inferior', 'inadequate', 'unsatisfactory',\n",
        "            'disappointing', 'unfortunate', 'regrettable', 'unpleasant', 'negative',\n",
        "            'problematic', 'troublesome', 'deficient', 'lacking', 'weak'\n",
        "        ]\n",
        "\n",
        "        # Weak negative sentiment\n",
        "        weak_negative = [\n",
        "            'mediocre', 'average', 'ordinary', 'unremarkable', 'forgettable', 'bland',\n",
        "            'boring', 'dull', 'tedious', 'monotonous'\n",
        "        ]\n",
        "\n",
        "        # Emotional states\n",
        "        emotions = [\n",
        "            'happy', 'sad', 'angry', 'frustrated', 'annoyed', 'irritated', 'furious',\n",
        "            'pleased', 'satisfied', 'content', 'delighted', 'thrilled', 'excited',\n",
        "            'disappointed', 'upset', 'distressed', 'concerned', 'worried', 'anxious',\n",
        "            'confused', 'surprised', 'shocked', 'amazed', 'grateful', 'thankful',\n",
        "            'relieved', 'hopeful', 'optimistic', 'pessimistic', 'discouraged'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # INTENSITY MODIFIERS (ADVERBS)\n",
        "        # =============================================================================\n",
        "\n",
        "        # Amplifiers (intensify sentiment)\n",
        "        amplifiers = [\n",
        "            'very', 'extremely', 'incredibly', 'absolutely', 'completely', 'totally',\n",
        "            'utterly', 'thoroughly', 'entirely', 'fully', 'highly', 'remarkably',\n",
        "            'exceptionally', 'extraordinarily', 'particularly', 'especially', 'truly',\n",
        "            'genuinely', 'really', 'seriously', 'desperately', 'severely', 'deeply',\n",
        "            'profoundly', 'intensely', 'immensely', 'tremendously', 'enormously'\n",
        "        ]\n",
        "\n",
        "        # Diminishers (reduce sentiment)\n",
        "        diminishers = [\n",
        "            'slightly', 'somewhat', 'fairly', 'rather', 'quite', 'pretty',\n",
        "            'relatively', 'moderately', 'reasonably', 'partially', 'partly',\n",
        "            'barely', 'hardly', 'scarcely', 'marginally', 'minimally', 'nominally'\n",
        "        ]\n",
        "\n",
        "        # Frequency adverbs\n",
        "        frequency = [\n",
        "            'always', 'constantly', 'continually', 'frequently', 'often', 'regularly',\n",
        "            'usually', 'normally', 'typically', 'generally', 'commonly', 'sometimes',\n",
        "            'occasionally', 'rarely', 'seldom', 'never', 'hardly ever', 'repeatedly',\n",
        "            'consistently', 'persistently', 'routinely'\n",
        "        ]\n",
        "\n",
        "        # Temporal adverbs\n",
        "        temporal = [\n",
        "            'now', 'currently', 'presently', 'today', 'recently', 'lately', 'yesterday',\n",
        "            'previously', 'formerly', 'earlier', 'soon', 'immediately', 'instantly',\n",
        "            'quickly', 'rapidly', 'swiftly', 'slowly', 'gradually', 'eventually',\n",
        "            'finally', 'ultimately', 'already', 'still', 'yet', 'anymore'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # NEGATION & CONTRAST\n",
        "        # =============================================================================\n",
        "\n",
        "        # Negation words\n",
        "        negations = [\n",
        "            'not', 'no', 'never', 'neither', 'nobody', 'nothing', 'nowhere',\n",
        "            'none', \"n't\", \"won't\", \"can't\", \"don't\", \"doesn't\", \"didn't\",\n",
        "            \"hasn't\", \"haven't\", \"hadn't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\",\n",
        "            \"wouldn't\", \"shouldn't\", \"couldn't\", \"mightn't\", \"mustn't\"\n",
        "        ]\n",
        "\n",
        "        # Contrast/adversative conjunctions\n",
        "        contrast_words = [\n",
        "            'but', 'however', 'although', 'though', 'yet', 'nevertheless',\n",
        "            'nonetheless', 'whereas', 'while', 'despite', 'except', 'unfortunately',\n",
        "            'sadly', 'regrettably', 'conversely', 'instead', 'rather', 'alternatively'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # CUSTOMER SERVICE & EXPERIENCE VOCABULARY\n",
        "        # =============================================================================\n",
        "\n",
        "        # Service quality descriptors\n",
        "        service_quality = [\n",
        "            'service', 'support', 'assistance', 'help', 'care', 'attention',\n",
        "            'response', 'resolution', 'solution', 'handling', 'treatment',\n",
        "            'professionalism', 'courtesy', 'politeness', 'friendliness', 'helpfulness',\n",
        "            'efficiency', 'effectiveness', 'competence', 'expertise', 'knowledge',\n",
        "            'responsiveness', 'availability', 'accessibility', 'reliability'\n",
        "        ]\n",
        "\n",
        "        # Customer experience terms\n",
        "        experience_terms = [\n",
        "            'experience', 'interaction', 'engagement', 'encounter', 'visit',\n",
        "            'journey', 'process', 'procedure', 'transaction', 'communication',\n",
        "            'correspondence', 'conversation', 'discussion', 'consultation', 'meeting'\n",
        "        ]\n",
        "\n",
        "        # Problem/issue terminology\n",
        "        problems = [\n",
        "            'problem', 'issue', 'trouble', 'difficulty', 'challenge', 'concern',\n",
        "            'complaint', 'grievance', 'dispute', 'conflict', 'matter', 'situation',\n",
        "            'complication', 'obstacle', 'hindrance', 'impediment', 'setback',\n",
        "            'malfunction', 'failure', 'error', 'mistake', 'bug', 'glitch',\n",
        "            'defect', 'flaw', 'fault', 'breakdown', 'outage', 'disruption'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # TELCO/TELECOM SPECIFIC VOCABULARY\n",
        "        # =============================================================================\n",
        "\n",
        "        # Network & connectivity\n",
        "        network_terms = [\n",
        "            'network', 'connection', 'connectivity', 'signal', 'coverage', 'reception',\n",
        "            'bandwidth', 'speed', 'latency', 'lag', 'delay', 'buffering',\n",
        "            'streaming', 'download', 'upload', 'throughput', 'quality',\n",
        "            'stability', 'reliability', 'availability', 'uptime', 'downtime',\n",
        "            'outage', 'interruption', 'disruption', 'interference'\n",
        "        ]\n",
        "\n",
        "        # Service types\n",
        "        telco_services = [\n",
        "            'phone', 'mobile', 'cellular', 'landline', 'telephone', 'call', 'calling',\n",
        "            'internet', 'broadband', 'wifi', 'wireless', 'data', 'roaming',\n",
        "            'voicemail', 'text', 'messaging', 'sms', 'mms', 'email',\n",
        "            'tv', 'television', 'cable', 'satellite', 'streaming', 'video',\n",
        "            'bundle', 'package', 'plan', 'subscription', 'contract', 'agreement'\n",
        "        ]\n",
        "\n",
        "        # Technical issues\n",
        "        technical_issues = [\n",
        "            'dropped', 'disconnected', 'lost', 'dead', 'frozen', 'stuck',\n",
        "            'slow', 'sluggish', 'intermittent', 'unstable', 'unreliable',\n",
        "            'spotty', 'patchy', 'inconsistent', 'degraded', 'throttled',\n",
        "            'blocked', 'restricted', 'limited', 'capped', 'overcharged'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # BILLING & PRICING VOCABULARY\n",
        "        # =============================================================================\n",
        "\n",
        "        # Financial terms\n",
        "        billing_terms = [\n",
        "            'bill', 'billing', 'charge', 'charges', 'fee', 'fees', 'cost', 'costs',\n",
        "            'price', 'pricing', 'rate', 'rates', 'payment', 'invoice', 'statement',\n",
        "            'balance', 'amount', 'total', 'subtotal', 'tax', 'taxes',\n",
        "            'discount', 'promotion', 'offer', 'deal', 'rebate', 'refund',\n",
        "            'credit', 'debit', 'overcharge', 'undercharge', 'adjustment'\n",
        "        ]\n",
        "\n",
        "        # Value perception\n",
        "        value_terms = [\n",
        "            'value', 'worth', 'worthwhile', 'affordable', 'expensive', 'cheap',\n",
        "            'costly', 'pricey', 'overpriced', 'underpriced', 'reasonable', 'fair',\n",
        "            'unfair', 'excessive', 'exorbitant', 'competitive', 'economical',\n",
        "            'budget', 'premium', 'luxury', 'standard', 'basic'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # CUSTOMER ACTIONS & INTENTIONS\n",
        "        # =============================================================================\n",
        "\n",
        "        # Churn signals (HIGH PRIORITY)\n",
        "        churn_signals = [\n",
        "            'cancel', 'canceling', 'cancelled', 'cancellation', 'terminate',\n",
        "            'terminating', 'terminated', 'termination', 'discontinue', 'disconnect',\n",
        "            'leave', 'leaving', 'left', 'quit', 'quitting', 'switch', 'switching',\n",
        "            'switched', 'change', 'changing', 'changed', 'move', 'moving', 'moved',\n",
        "            'transfer', 'transferring', 'end', 'ending', 'ended', 'stop', 'stopping',\n",
        "            'stopped', 'drop', 'dropping', 'dropped'\n",
        "        ]\n",
        "\n",
        "        # Retention signals\n",
        "        retention_signals = [\n",
        "            'stay', 'staying', 'stayed', 'remain', 'remaining', 'remained',\n",
        "            'continue', 'continuing', 'continued', 'renew', 'renewing', 'renewed',\n",
        "            'extend', 'extending', 'extended', 'upgrade', 'upgrading', 'upgraded',\n",
        "            'keep', 'keeping', 'kept', 'retain', 'retaining', 'retained'\n",
        "        ]\n",
        "\n",
        "        # Contact/engagement actions\n",
        "        engagement_actions = [\n",
        "            'contact', 'contacted', 'contacting', 'call', 'called', 'calling',\n",
        "            'email', 'emailed', 'emailing', 'message', 'messaged', 'messaging',\n",
        "            'chat', 'chatted', 'chatting', 'speak', 'spoke', 'spoken', 'speaking',\n",
        "            'talk', 'talked', 'talking', 'reach', 'reached', 'reaching',\n",
        "            'report', 'reported', 'reporting', 'complain', 'complained', 'complaining',\n",
        "            'request', 'requested', 'requesting', 'ask', 'asked', 'asking'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # COMPARISON & COMPETITOR VOCABULARY\n",
        "        # =============================================================================\n",
        "\n",
        "        # Competitor mentions\n",
        "        competitor_terms = [\n",
        "            'competitor', 'competition', 'rival', 'alternative', 'option',\n",
        "            'other', 'another', 'different', 'elsewhere', 'switch', 'compare',\n",
        "            'comparison', 'versus', 'vs', 'better', 'worse', 'superior',\n",
        "            'inferior', 'prefer', 'preference', 'choice'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # TEMPORAL EXPRESSIONS\n",
        "        # =============================================================================\n",
        "\n",
        "        # Duration\n",
        "        duration_terms = [\n",
        "            'second', 'seconds', 'minute', 'minutes', 'hour', 'hours',\n",
        "            'day', 'days', 'week', 'weeks', 'month', 'months', 'year', 'years',\n",
        "            'long', 'short', 'brief', 'extended', 'prolonged', 'temporary',\n",
        "            'permanent', 'ongoing', 'continuous'\n",
        "        ]\n",
        "\n",
        "        # Time references\n",
        "        time_references = [\n",
        "            'ago', 'since', 'until', 'till', 'from', 'to', 'between',\n",
        "            'during', 'within', 'after', 'before', 'past', 'future',\n",
        "            'present', 'current', 'previous', 'next', 'last', 'first'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # STANDARD LINGUISTIC CATEGORIES\n",
        "        # =============================================================================\n",
        "\n",
        "        # Common verbs\n",
        "        common_verbs = [\n",
        "            'is', 'am', 'are', 'was', 'were', 'be', 'been', 'being',\n",
        "            'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
        "            'make', 'makes', 'made', 'making', 'get', 'gets', 'got', 'getting',\n",
        "            'go', 'goes', 'went', 'going', 'gone', 'come', 'comes', 'came', 'coming',\n",
        "            'take', 'takes', 'took', 'taking', 'taken', 'see', 'sees', 'saw', 'seeing', 'seen',\n",
        "            'know', 'knows', 'knew', 'knowing', 'known', 'think', 'thinks', 'thought', 'thinking',\n",
        "            'give', 'gives', 'gave', 'giving', 'given', 'find', 'finds', 'found', 'finding',\n",
        "            'tell', 'tells', 'told', 'telling', 'become', 'becomes', 'became', 'becoming',\n",
        "            'show', 'shows', 'showed', 'showing', 'shown', 'let', 'lets', 'letting',\n",
        "            'begin', 'begins', 'began', 'beginning', 'begun', 'seem', 'seems', 'seemed', 'seeming',\n",
        "            'help', 'helps', 'helped', 'helping', 'try', 'tries', 'tried', 'trying',\n",
        "            'use', 'uses', 'used', 'using', 'need', 'needs', 'needed', 'needing',\n",
        "            'want', 'wants', 'wanted', 'wanting', 'work', 'works', 'worked', 'working',\n",
        "            'feel', 'feels', 'felt', 'feeling', 'become', 'becomes', 'became', 'becoming',\n",
        "            'provide', 'provides', 'provided', 'providing', 'lose', 'loses', 'lost', 'losing',\n",
        "            'pay', 'pays', 'paid', 'paying', 'meet', 'meets', 'met', 'meeting',\n",
        "            'include', 'includes', 'included', 'including', 'continue', 'continues', 'continued',\n",
        "            'set', 'sets', 'setting', 'learn', 'learns', 'learned', 'learning',\n",
        "            'add', 'adds', 'added', 'adding', 'understand', 'understands', 'understood', 'understanding'\n",
        "        ]\n",
        "\n",
        "        # Common nouns\n",
        "        common_nouns = [\n",
        "            'time', 'person', 'people', 'year', 'way', 'day', 'thing', 'man', 'woman',\n",
        "            'world', 'life', 'hand', 'part', 'child', 'children', 'eye', 'place', 'work',\n",
        "            'week', 'case', 'point', 'government', 'company', 'number', 'group', 'fact',\n",
        "            'water', 'room', 'money', 'story', 'book', 'movie', 'car', 'house', 'food',\n",
        "            'music', 'idea', 'business', 'system', 'program', 'question', 'information',\n",
        "            'family', 'friend', 'school', 'student', 'game', 'team', 'job', 'city',\n",
        "            'country', 'state', 'community', 'area', 'result', 'change', 'product',\n",
        "            'market', 'customer', 'client', 'member', 'account', 'user', 'representative',\n",
        "            'agent', 'manager', 'supervisor', 'department', 'office', 'center', 'store'\n",
        "        ]\n",
        "\n",
        "        # Common adjectives\n",
        "        common_adjectives = [\n",
        "            'new', 'old', 'high', 'low', 'big', 'small', 'large', 'little', 'long', 'short',\n",
        "            'early', 'late', 'young', 'important', 'different', 'same', 'right', 'wrong',\n",
        "            'able', 'unable', 'certain', 'possible', 'impossible', 'available', 'unavailable',\n",
        "            'full', 'empty', 'whole', 'complete', 'incomplete', 'open', 'closed',\n",
        "            'public', 'private', 'personal', 'professional', 'social', 'economic',\n",
        "            'political', 'national', 'international', 'local', 'global', 'general',\n",
        "            'specific', 'particular', 'special', 'normal', 'regular', 'standard',\n",
        "            'simple', 'complex', 'easy', 'difficult', 'hard', 'clear', 'unclear',\n",
        "            'strong', 'weak', 'free', 'busy', 'ready', 'sure', 'unsure'\n",
        "        ]\n",
        "\n",
        "        # Pronouns\n",
        "        pronouns = [\n",
        "            'i', 'me', 'my', 'mine', 'myself',\n",
        "            'you', 'your', 'yours', 'yourself', 'yourselves',\n",
        "            'he', 'him', 'his', 'himself',\n",
        "            'she', 'her', 'hers', 'herself',\n",
        "            'it', 'its', 'itself',\n",
        "            'we', 'us', 'our', 'ours', 'ourselves',\n",
        "            'they', 'them', 'their', 'theirs', 'themselves',\n",
        "            'this', 'that', 'these', 'those',\n",
        "            'who', 'whom', 'whose', 'which', 'what',\n",
        "            'anybody', 'anyone', 'anything', 'everybody', 'everyone', 'everything',\n",
        "            'somebody', 'someone', 'something', 'nobody', 'none', 'nothing'\n",
        "        ]\n",
        "\n",
        "        # Prepositions\n",
        "        prepositions = [\n",
        "            'of', 'in', 'to', 'for', 'with', 'on', 'at', 'from', 'by', 'about',\n",
        "            'as', 'into', 'like', 'through', 'after', 'over', 'between', 'out',\n",
        "            'against', 'during', 'without', 'before', 'under', 'around', 'among',\n",
        "            'beneath', 'beside', 'below', 'above', 'across', 'behind', 'beyond',\n",
        "            'plus', 'except', 'near', 'off', 'per', 'regarding', 'since', 'than',\n",
        "            'toward', 'towards', 'upon', 'within', 'via', 'throughout'\n",
        "        ]\n",
        "\n",
        "        # Conjunctions\n",
        "        conjunctions = [\n",
        "            'and', 'or', 'but', 'if', 'because', 'as', 'while', 'when', 'where',\n",
        "            'although', 'though', 'unless', 'until', 'since', 'so', 'whether',\n",
        "            'nor', 'yet', 'either', 'neither', 'both'\n",
        "        ]\n",
        "\n",
        "        # Determiners\n",
        "        determiners = [\n",
        "            'the', 'a', 'an', 'this', 'that', 'these', 'those', 'my', 'your',\n",
        "            'his', 'her', 'its', 'our', 'their', 'some', 'any', 'all', 'each',\n",
        "            'every', 'no', 'many', 'much', 'few', 'little', 'several', 'most',\n",
        "            'more', 'less', 'fewer', 'other', 'another', 'such', 'own'\n",
        "        ]\n",
        "\n",
        "        # Modal verbs\n",
        "        modals = [\n",
        "            'can', 'could', 'may', 'might', 'must', 'shall', 'should',\n",
        "            'will', 'would', 'ought', 'need', 'dare'\n",
        "        ]\n",
        "\n",
        "        # Numbers and quantifiers\n",
        "        numbers = [str(i) for i in range(0, 101)] + [\n",
        "            'hundred', 'thousand', 'million', 'billion', 'trillion',\n",
        "            'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven',\n",
        "            'eight', 'nine', 'ten', 'first', 'second', 'third', 'fourth',\n",
        "            'fifth', 'once', 'twice', 'double', 'triple', 'half', 'quarter',\n",
        "            'dozen', 'couple', 'multiple', 'single', 'numerous', 'countless'\n",
        "        ]\n",
        "\n",
        "        # Question words\n",
        "        question_words = [\n",
        "            'who', 'what', 'when', 'where', 'why', 'how', 'which', 'whose',\n",
        "            'whom', 'whatever', 'whenever', 'wherever', 'however', 'whichever'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # PUNCTUATION & SPECIAL TOKENS\n",
        "        # =============================================================================\n",
        "\n",
        "        punctuation = [\n",
        "            '.', ',', '!', '?', ';', ':', '-', '--', '‚Äî', '(', ')', '[', ']',\n",
        "            '{', '}', '\"', \"'\", '`', '/', '\\\\', '|', '@', '#', '$', '%', '&',\n",
        "            '*', '+', '=', '<', '>', '~', '^'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # COMBINE ALL VOCABULARIES\n",
        "        # =============================================================================\n",
        "\n",
        "        all_word_lists = [\n",
        "            # Sentiment\n",
        "            strong_positive, moderate_positive, weak_positive,\n",
        "            strong_negative, moderate_negative, weak_negative, emotions,\n",
        "            # Modifiers\n",
        "            amplifiers, diminishers, frequency, temporal,\n",
        "            # Negation & contrast\n",
        "            negations, contrast_words,\n",
        "            # Customer service\n",
        "            service_quality, experience_terms, problems,\n",
        "            # Telco specific\n",
        "            network_terms, telco_services, technical_issues,\n",
        "            # Billing\n",
        "            billing_terms, value_terms,\n",
        "            # Actions\n",
        "            churn_signals, retention_signals, engagement_actions,\n",
        "            # Comparison\n",
        "            competitor_terms,\n",
        "            # Temporal\n",
        "            duration_terms, time_references,\n",
        "            # Standard linguistic\n",
        "            common_verbs, common_nouns, common_adjectives,\n",
        "            pronouns, prepositions, conjunctions, determiners,\n",
        "            modals, numbers, question_words, punctuation\n",
        "        ]\n",
        "\n",
        "        # Flatten and remove duplicates\n",
        "        self.vocab_words = []\n",
        "        for word_list in all_word_lists:\n",
        "            self.vocab_words.extend(word_list)\n",
        "\n",
        "        self.vocab_words = sorted(list(set(self.vocab_words)))\n",
        "\n",
        "        # Build mappings with special tokens\n",
        "        self.word_to_idx = {\n",
        "            '<PAD>': 0,\n",
        "            '<UNK>': 1,\n",
        "            '<SOS>': 2,  # Start of sequence\n",
        "            '<EOS>': 3,  # End of sequence\n",
        "        }\n",
        "\n",
        "        for i, word in enumerate(self.vocab_words, start=4):\n",
        "            self.word_to_idx[word] = i\n",
        "\n",
        "        self.idx_to_word = {v: k for k, v in self.word_to_idx.items()}\n",
        "        self.vocab_size = len(self.word_to_idx)\n",
        "\n",
        "        # Create semantic category mappings for interpretability\n",
        "        self.semantic_categories = {\n",
        "            'strong_positive': set(strong_positive),\n",
        "            'moderate_positive': set(moderate_positive),\n",
        "            'weak_positive': set(weak_positive),\n",
        "            'strong_negative': set(strong_negative),\n",
        "            'moderate_negative': set(moderate_negative),\n",
        "            'weak_negative': set(weak_negative),\n",
        "            'emotions': set(emotions),\n",
        "            'amplifiers': set(amplifiers),\n",
        "            'diminishers': set(diminishers),\n",
        "            'negations': set(negations),\n",
        "            'churn_signals': set(churn_signals),\n",
        "            'retention_signals': set(retention_signals),\n",
        "            'problems': set(problems),\n",
        "            'network_terms': set(network_terms),\n",
        "            'billing_terms': set(billing_terms),\n",
        "        }\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        \"\"\"Convert text to token IDs.\"\"\"\n",
        "        text = text.lower()\n",
        "        # Simple whitespace and punctuation tokenization\n",
        "        import re\n",
        "        # Split on whitespace and keep punctuation\n",
        "        tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
        "        return [self.word_to_idx.get(token, self.word_to_idx['<UNK>']) for token in tokens]\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        \"\"\"Convert token IDs back to text.\"\"\"\n",
        "        words = [self.idx_to_word.get(idx, '<UNK>') for idx in ids]\n",
        "        # Simple detokenization\n",
        "        text = ' '.join(words)\n",
        "        # Fix punctuation spacing\n",
        "        import re\n",
        "        text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n",
        "        text = re.sub(r'\\(\\s+', '(', text)\n",
        "        text = re.sub(r'\\s+\\)', ')', text)\n",
        "        return text\n",
        "\n",
        "    def get_word_category(self, word: str) -> List[str]:\n",
        "        \"\"\"Return all semantic categories a word belongs to.\"\"\"\n",
        "        word = word.lower()\n",
        "        categories = []\n",
        "        for cat_name, cat_words in self.semantic_categories.items():\n",
        "            if word in cat_words:\n",
        "                categories.append(cat_name)\n",
        "        return categories if categories else ['other']\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict:\n",
        "        \"\"\"Analyze text and return category breakdown.\"\"\"\n",
        "        tokens = text.lower().split()\n",
        "        category_counts = {cat: 0 for cat in self.semantic_categories.keys()}\n",
        "        category_counts['other'] = 0\n",
        "\n",
        "        for token in tokens:\n",
        "            categories = self.get_word_category(token)\n",
        "            for cat in categories:\n",
        "                category_counts[cat] += 1\n",
        "\n",
        "        return category_counts\n",
        "\n",
        "    def get_vocab_stats(self):\n",
        "        \"\"\"Print comprehensive vocabulary statistics.\"\"\"\n",
        "        print(f\"üìö Comprehensive Churn Tokenizer Statistics:\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"   Total vocabulary size: {self.vocab_size:,} tokens\")\n",
        "        print(f\"   Content words: {len(self.vocab_words):,}\")\n",
        "        print(f\"\\n   üìä Category Breakdown:\")\n",
        "\n",
        "        category_sizes = {\n",
        "            name: len(words)\n",
        "            for name, words in self.semantic_categories.items()\n",
        "        }\n",
        "\n",
        "        for cat_name, size in sorted(category_sizes.items(), key=lambda x: -x[1])[:15]:\n",
        "            print(f\"      {cat_name:<25} : {size:>4} words\")\n",
        "\n",
        "        print(f\"\\n   üî§ Sample tokens (first 30):\")\n",
        "        for i, word in enumerate(self.vocab_words[:30]):\n",
        "            categories = self.get_word_category(word)\n",
        "            cat_str = ', '.join(categories[:2])  # Show first 2 categories\n",
        "            print(f\"      {i+4:>4}: '{word:<20}' [{cat_str}]\")\n",
        "        print(f\"      ...\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 2B: COMPREHENSIVE CHURN DATASET\n",
        "# =============================================================================\n",
        "def create_comprehensive_churn_dataset():\n",
        "    \"\"\"\n",
        "    Create realistic customer churn dataset with diverse scenarios.\n",
        "    \"\"\"\n",
        "\n",
        "    # HIGH CHURN RISK - Negative texts (label = 0)\n",
        "    high_churn_texts = [\n",
        "        # Direct cancellation intent\n",
        "        \"i want to cancel my service\",\n",
        "        \"please cancel my account immediately\",\n",
        "        \"i am cancelling my subscription today\",\n",
        "        \"need to terminate my contract\",\n",
        "        \"i would like to discontinue service\",\n",
        "\n",
        "        # Switching to competitor\n",
        "        \"switching to another provider next month\",\n",
        "        \"found better deal with competitor\",\n",
        "        \"moving to different company\",\n",
        "        \"competitor offers better service\",\n",
        "        \"leaving for cheaper alternative\",\n",
        "\n",
        "        # Service quality complaints\n",
        "        \"terrible network coverage in my area\",\n",
        "        \"internet speed is extremely slow\",\n",
        "        \"dropped calls constantly\",\n",
        "        \"connection keeps disconnecting\",\n",
        "        \"service is completely unreliable\",\n",
        "        \"network outage every single day\",\n",
        "\n",
        "        # Billing complaints\n",
        "        \"bills are way too expensive\",\n",
        "        \"overcharged again this month\",\n",
        "        \"hidden fees everywhere\",\n",
        "        \"billing errors every month\",\n",
        "        \"price increased without notice\",\n",
        "\n",
        "        # Customer service complaints\n",
        "        \"customer service is absolutely horrible\",\n",
        "        \"waited hours for support\",\n",
        "        \"representatives are very rude\",\n",
        "        \"nobody helps with my problems\",\n",
        "        \"worst customer service ever\",\n",
        "\n",
        "        # Frustrated with ongoing issues\n",
        "        \"nothing works properly anymore\",\n",
        "        \"tired of dealing with constant problems\",\n",
        "        \"same issue for months now\",\n",
        "        \"completely fed up with service\",\n",
        "        \"this is getting ridiculous\",\n",
        "\n",
        "        # Complex negative scenarios\n",
        "        \"internet drops every hour and support does not help\",\n",
        "        \"paying too much for terrible service quality\",\n",
        "        \"been customer for years but treated poorly\",\n",
        "        \"promised better service but got worse\",\n",
        "        \"completely disappointed with everything\",\n",
        "    ]\n",
        "\n",
        "    # LOW CHURN RISK - Positive texts (label = 1)\n",
        "    low_churn_texts = [\n",
        "        # Satisfaction expressions\n",
        "        \"very happy with my service\",\n",
        "        \"excellent network coverage\",\n",
        "        \"great value for money\",\n",
        "        \"super reliable connection\",\n",
        "        \"fast internet speed always\",\n",
        "\n",
        "        # Positive service experiences\n",
        "        \"customer support was very helpful\",\n",
        "        \"representative solved my problem quickly\",\n",
        "        \"easy to contact support team\",\n",
        "        \"friendly and professional service\",\n",
        "        \"issue resolved immediately\",\n",
        "\n",
        "        # Loyalty signals\n",
        "        \"been customer for years\",\n",
        "        \"staying with this provider\",\n",
        "        \"recently upgraded my plan\",\n",
        "        \"renewed my contract\",\n",
        "        \"recommended to family and friends\",\n",
        "\n",
        "        # Positive comparisons\n",
        "        \"much better than previous provider\",\n",
        "        \"best service in the area\",\n",
        "        \"no complaints at all\",\n",
        "        \"everything works perfectly\",\n",
        "        \"consistently good experience\",\n",
        "\n",
        "        # Value appreciation\n",
        "        \"fair pricing for quality\",\n",
        "        \"good deals available\",\n",
        "        \"affordable monthly bill\",\n",
        "        \"worth every penny\",\n",
        "        \"competitive rates\",\n",
        "\n",
        "        # Quality praise\n",
        "        \"crystal clear call quality\",\n",
        "        \"blazing fast download speeds\",\n",
        "        \"stable connection always\",\n",
        "        \"never experienced outage\",\n",
        "        \"service exceeded expectations\",\n",
        "\n",
        "        # Complex positive scenarios\n",
        "        \"had minor issue but support fixed quickly\",\n",
        "        \"great service and reasonable price together\",\n",
        "        \"reliable network and excellent customer care\",\n",
        "        \"upgraded plan and very satisfied\",\n",
        "        \"longtime customer and still happy\",\n",
        "    ]\n",
        "\n",
        "    # Create balanced dataset\n",
        "    texts = high_churn_texts + low_churn_texts\n",
        "    labels = [0] * len(high_churn_texts) + [1] * len(low_churn_texts)\n",
        "\n",
        "    # Shuffle\n",
        "    indices = torch.randperm(len(texts))\n",
        "    texts = [texts[i] for i in indices]\n",
        "    labels = [labels[i] for i in indices]\n",
        "\n",
        "    print(f\"üìä Comprehensive Churn Dataset Created:\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Total examples: {len(texts)}\")\n",
        "    print(f\"   High churn risk (0): {sum(1 for l in labels if l == 0)}\")\n",
        "    print(f\"   Low churn risk (1): {sum(1 for l in labels if l == 1)}\")\n",
        "    print(f\"\\n   Sample examples:\")\n",
        "    for i in range(6):\n",
        "        risk = 'HIGH CHURN' if labels[i] == 0 else 'LOW CHURN'\n",
        "        print(f\"      [{risk}] {texts[i]}\")\n",
        "\n",
        "    return texts, labels"
      ],
      "metadata": {
        "id": "Biv9sjhpbfSr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 3: ENHANCED TRAINING WITH CATEGORY AWARENESS\n",
        "# =============================================================================\n",
        "class CategoryAwareSentimentTrainer:\n",
        "    \"\"\"\n",
        "    Enhanced trainer that tracks semantic category usage during training.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, tokenizer, max_len=64):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Track category importance over time\n",
        "        self.category_importance = {cat: [] for cat in tokenizer.semantic_categories.keys()}\n",
        "\n",
        "    def prepare_batch(self, texts: List[str], labels: List[int]):\n",
        "        \"\"\"Tokenize and pad texts.\"\"\"\n",
        "        encoded = []\n",
        "        for text in texts:\n",
        "            ids = self.tokenizer.encode(text)\n",
        "            # Pad or truncate\n",
        "            if len(ids) < self.max_len:\n",
        "                ids = ids + [0] * (self.max_len - len(ids))\n",
        "            else:\n",
        "                ids = ids[:self.max_len]\n",
        "            encoded.append(ids)\n",
        "\n",
        "        input_ids = torch.tensor(encoded, device=self.device)\n",
        "        labels_tensor = torch.tensor(labels, device=self.device)\n",
        "        return input_ids, labels_tensor\n",
        "\n",
        "    def train(self, texts: List[str], labels: List[int],\n",
        "              epochs: int = 50, lr: float = 0.001, batch_size: int = 8,\n",
        "              verbose: bool = True):\n",
        "        \"\"\"Train with category tracking.\"\"\"\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        # Learning rate scheduler for better convergence\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Training history\n",
        "        history = {\n",
        "            'loss': [],\n",
        "            'accuracy': [],\n",
        "            'category_usage': [],\n",
        "            'best_epoch': 0,\n",
        "            'best_accuracy': 0.0\n",
        "        }\n",
        "\n",
        "        if verbose:\n",
        "            print(\"üöÄ Starting Category-Aware Training\")\n",
        "            print(\"=\" * 70)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            epoch_category_counts = {cat: 0 for cat in self.tokenizer.semantic_categories.keys()}\n",
        "\n",
        "            # Mini-batch training\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch_texts = texts[i:i+batch_size]\n",
        "                batch_labels = labels[i:i+batch_size]\n",
        "\n",
        "                # Track category usage in this batch\n",
        "                for text in batch_texts:\n",
        "                    cat_counts = self.tokenizer.analyze_text(text)\n",
        "                    for cat, count in cat_counts.items():\n",
        "                        if cat in epoch_category_counts:\n",
        "                            epoch_category_counts[cat] += count\n",
        "\n",
        "                # Prepare batch\n",
        "                input_ids, label_tensor = self.prepare_batch(batch_texts, batch_labels)\n",
        "\n",
        "                # Forward pass\n",
        "                logits, interpretability = self.model(input_ids)\n",
        "\n",
        "                # Use last token's prediction for classification\n",
        "                logits_cls = logits[:, -1, :2]\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(logits_cls, label_tensor)\n",
        "\n",
        "                # Backward pass\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping for stability\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                # Track metrics\n",
        "                total_loss += loss.item()\n",
        "                predictions = torch.argmax(logits_cls, dim=-1)\n",
        "                correct += (predictions == label_tensor).sum().item()\n",
        "                total += len(batch_labels)\n",
        "\n",
        "            # Epoch metrics\n",
        "            avg_loss = total_loss / (len(texts) / batch_size)\n",
        "            accuracy = correct / total\n",
        "\n",
        "            # Update scheduler\n",
        "            scheduler.step(avg_loss)\n",
        "\n",
        "            # Track best model\n",
        "            if accuracy > history['best_accuracy']:\n",
        "                history['best_accuracy'] = accuracy\n",
        "                history['best_epoch'] = epoch + 1\n",
        "\n",
        "            history['loss'].append(avg_loss)\n",
        "            history['accuracy'].append(accuracy)\n",
        "            history['category_usage'].append(epoch_category_counts)\n",
        "\n",
        "            # Store category importance\n",
        "            for cat, count in epoch_category_counts.items():\n",
        "                if cat in self.category_importance:\n",
        "                    self.category_importance[cat].append(count)\n",
        "\n",
        "            if verbose and (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1:3d}/{epochs} | Loss: {avg_loss:.4f} | \"\n",
        "                      f\"Accuracy: {accuracy:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\n‚úÖ Training Complete!\")\n",
        "            print(f\"   Best Accuracy: {history['best_accuracy']:.4f} at Epoch {history['best_epoch']}\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    def evaluate(self, texts: List[str], labels: List[int], verbose: bool = True):\n",
        "        \"\"\"Evaluate model with detailed metrics.\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(texts), 8):\n",
        "                batch_texts = texts[i:i+8]\n",
        "                batch_labels = labels[i:i+8]\n",
        "\n",
        "                input_ids, label_tensor = self.prepare_batch(batch_texts, batch_labels)\n",
        "                logits, _ = self.model(input_ids)\n",
        "\n",
        "                logits_cls = logits[:, -1, :2]\n",
        "                predictions = torch.argmax(logits_cls, dim=-1)\n",
        "\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "                all_labels.extend(label_tensor.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        all_predictions = np.array(all_predictions)\n",
        "        all_labels = np.array(all_labels)\n",
        "\n",
        "        accuracy = (all_predictions == all_labels).mean()\n",
        "\n",
        "        # Confusion matrix\n",
        "        tp = ((all_predictions == 1) & (all_labels == 1)).sum()\n",
        "        tn = ((all_predictions == 0) & (all_labels == 0)).sum()\n",
        "        fp = ((all_predictions == 1) & (all_labels == 0)).sum()\n",
        "        fn = ((all_predictions == 0) & (all_labels == 1)).sum()\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nüìä Evaluation Results:\")\n",
        "            print(f\"{'='*70}\")\n",
        "            print(f\"   Accuracy:  {accuracy:.4f}\")\n",
        "            print(f\"   Precision: {precision:.4f}\")\n",
        "            print(f\"   Recall:    {recall:.4f}\")\n",
        "            print(f\"   F1-Score:  {f1:.4f}\")\n",
        "            print(f\"\\n   Confusion Matrix:\")\n",
        "            print(f\"      True Neg:  {tn:3d}  |  False Pos: {fp:3d}\")\n",
        "            print(f\"      False Neg: {fn:3d}  |  True Pos:  {tp:3d}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'confusion_matrix': {'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn}\n",
        "        }\n",
        "\n",
        "    def analyze_predictions(self, texts: List[str], labels: List[int], top_n: int = 5):\n",
        "        \"\"\"Analyze model predictions with category breakdown.\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        results = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for text, label in zip(texts, labels):\n",
        "                input_ids = torch.tensor([self.tokenizer.encode(text)], device=self.device)\n",
        "                if input_ids.shape[1] < self.max_len:\n",
        "                    input_ids = F.pad(input_ids, (0, self.max_len - input_ids.shape[1]))\n",
        "                else:\n",
        "                    input_ids = input_ids[:, :self.max_len]\n",
        "\n",
        "                logits, interp = self.model(input_ids)\n",
        "                logits_cls = logits[:, -1, :2]\n",
        "                prediction = torch.argmax(logits_cls, dim=-1).item()\n",
        "                confidence = torch.softmax(logits_cls, dim=-1)[0, prediction].item()\n",
        "\n",
        "                # Analyze text categories\n",
        "                categories = self.tokenizer.analyze_text(text)\n",
        "                active_categories = {k: v for k, v in categories.items() if v > 0}\n",
        "\n",
        "                results.append({\n",
        "                    'text': text,\n",
        "                    'true_label': label,\n",
        "                    'prediction': prediction,\n",
        "                    'confidence': confidence,\n",
        "                    'correct': prediction == label,\n",
        "                    'categories': active_categories\n",
        "                })\n",
        "\n",
        "        # Print top misclassifications\n",
        "        misclassified = [r for r in results if not r['correct']]\n",
        "\n",
        "        print(f\"\\nüîç Prediction Analysis:\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"   Total samples: {len(results)}\")\n",
        "        print(f\"   Correct: {sum(r['correct'] for r in results)}\")\n",
        "        print(f\"   Incorrect: {len(misclassified)}\")\n",
        "\n",
        "        if misclassified:\n",
        "            print(f\"\\n   Top {min(top_n, len(misclassified))} Misclassifications:\")\n",
        "            for i, result in enumerate(misclassified[:top_n], 1):\n",
        "                true_label = 'LOW CHURN' if result['true_label'] == 1 else 'HIGH CHURN'\n",
        "                pred_label = 'LOW CHURN' if result['prediction'] == 1 else 'HIGH CHURN'\n",
        "                print(f\"\\n   {i}. Text: '{result['text']}'\")\n",
        "                print(f\"      True: {true_label} | Predicted: {pred_label} | Confidence: {result['confidence']:.3f}\")\n",
        "                print(f\"      Categories: {result['categories']}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_category_importance(self):\n",
        "        \"\"\"Visualize which semantic categories are most important.\"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Calculate average usage per category\n",
        "        avg_usage = {\n",
        "            cat: np.mean(counts) if counts else 0\n",
        "            for cat, counts in self.category_importance.items()\n",
        "        }\n",
        "\n",
        "        # Sort by importance\n",
        "        sorted_cats = sorted(avg_usage.items(), key=lambda x: -x[1])[:15]\n",
        "\n",
        "        categories = [cat for cat, _ in sorted_cats]\n",
        "        values = [val for _, val in sorted_cats]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        ax.barh(categories, values, color='steelblue')\n",
        "        ax.set_xlabel('Average Token Count per Epoch')\n",
        "        ax.set_title('Most Frequently Used Semantic Categories')\n",
        "        ax.invert_yaxis()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return sorted_cats\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 4: COMPLETE TRAINING PIPELINE\n",
        "# =============================================================================\n",
        "def run_comprehensive_training():\n",
        "    \"\"\"Complete training pipeline with all enhancements.\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GLASS BOX TRANSFORMER - COMPREHENSIVE CHURN PREDICTION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Step 1: Create tokenizer\n",
        "    print(\"\\nüìñ Step 1: Creating Comprehensive Tokenizer\")\n",
        "    print(\"-\"*70)\n",
        "    tokenizer = ComprehensiveChurnTokenizer()\n",
        "    tokenizer.get_vocab_stats()\n",
        "\n",
        "    # Step 2: Create dataset\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä Step 2: Creating Churn Dataset\")\n",
        "    print(\"-\"*70)\n",
        "    texts, labels = create_comprehensive_churn_dataset()\n",
        "\n",
        "    # Split into train/test\n",
        "    split_idx = int(0.8 * len(texts))\n",
        "    train_texts, test_texts = texts[:split_idx], texts[split_idx:]\n",
        "    train_labels, test_labels = labels[:split_idx], labels[split_idx:]\n",
        "\n",
        "    print(f\"\\n   Train set: {len(train_texts)} examples\")\n",
        "    print(f\"   Test set:  {len(test_texts)} examples\")\n",
        "\n",
        "    # Step 3: Create model\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üèóÔ∏è  Step 3: Creating Glass Box Transformer\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    model = GlassBoxTransformer(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        d_model=128,\n",
        "        n_layers=4,\n",
        "        n_heads=4,\n",
        "        d_ff=512,\n",
        "        max_seq_len=64\n",
        "    )\n",
        "\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"   ‚úì Model initialized\")\n",
        "    print(f\"   ‚úì Parameters: {n_params:,}\")\n",
        "    print(f\"   ‚úì Model size: ~{n_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "    # Step 4: Train model\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üöÄ Step 4: Training Model\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    trainer = CategoryAwareSentimentTrainer(model, tokenizer, max_len=64)\n",
        "    history = trainer.train(\n",
        "        train_texts,\n",
        "        train_labels,\n",
        "        epochs=50,\n",
        "        lr=0.001,\n",
        "        batch_size=8,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Step 5: Evaluate\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìà Step 5: Evaluating Model\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    print(\"\\n   Training Set Performance:\")\n",
        "    train_metrics = trainer.evaluate(train_texts, train_labels, verbose=True)\n",
        "\n",
        "    print(\"\\n   Test Set Performance:\")\n",
        "    test_metrics = trainer.evaluate(test_texts, test_labels, verbose=True)\n",
        "\n",
        "    # Step 6: Analyze predictions\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üî¨ Step 6: Analyzing Predictions\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    results = trainer.analyze_predictions(test_texts, test_labels, top_n=5)\n",
        "\n",
        "    # Step 7: Visualize category importance\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä Step 7: Category Importance Analysis\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    top_categories = trainer.plot_category_importance()\n",
        "\n",
        "    print(f\"\\n   Top 10 Most Important Categories:\")\n",
        "    for i, (cat, count) in enumerate(top_categories[:10], 1):\n",
        "        print(f\"      {i:2d}. {cat:<25} : {count:.1f} avg tokens/epoch\")\n",
        "\n",
        "    # Step 8: Test on new examples\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üß™ Step 8: Testing on New Examples\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    test_examples = [\n",
        "        \"i want to cancel my service immediately\",\n",
        "        \"very happy with the network coverage\",\n",
        "        \"terrible customer service experience\",\n",
        "        \"staying with this provider for years\",\n",
        "        \"switching to competitor next week\",\n",
        "        \"excellent value for the price\"\n",
        "    ]\n",
        "\n",
        "    model.eval()\n",
        "    print(\"\\n   Predictions:\")\n",
        "    with torch.no_grad():\n",
        "        for text in test_examples:\n",
        "            input_ids = torch.tensor([tokenizer.encode(text)], device=trainer.device)\n",
        "            if input_ids.shape[1] < 64:\n",
        "                input_ids = F.pad(input_ids, (0, 64 - input_ids.shape[1]))\n",
        "            else:\n",
        "                input_ids = input_ids[:, :64]\n",
        "\n",
        "            logits, _ = model(input_ids)\n",
        "            logits_cls = logits[:, -1, :2]\n",
        "            probs = torch.softmax(logits_cls, dim=-1)[0]\n",
        "            prediction = torch.argmax(probs).item()\n",
        "\n",
        "            churn_prob = probs[0].item()\n",
        "            retain_prob = probs[1].item()\n",
        "\n",
        "            label = \"üî¥ HIGH CHURN\" if prediction == 0 else \"üü¢ LOW CHURN\"\n",
        "\n",
        "            print(f\"\\n      Text: '{text}'\")\n",
        "            print(f\"      Prediction: {label}\")\n",
        "            print(f\"      Confidence: Churn={churn_prob:.3f} | Retain={retain_prob:.3f}\")\n",
        "\n",
        "            # Show category breakdown\n",
        "            categories = tokenizer.analyze_text(text)\n",
        "            active = {k: v for k, v in categories.items() if v > 0 and k != 'other'}\n",
        "            if active:\n",
        "                print(f\"      Categories: {active}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ TRAINING PIPELINE COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return model, tokenizer, trainer, history\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# RUN EVERYTHING\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    model, tokenizer, trainer, history = run_comprehensive_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ge4m-F5TbicY",
        "outputId": "995a38e4-0f17-48cc-bb48-2ae32fc76af8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "GLASS BOX TRANSFORMER - COMPREHENSIVE CHURN PREDICTION\n",
            "======================================================================\n",
            "\n",
            "üìñ Step 1: Creating Comprehensive Tokenizer\n",
            "----------------------------------------------------------------------\n",
            "üìö Comprehensive Churn Tokenizer Statistics:\n",
            "======================================================================\n",
            "   Total vocabulary size: 1,176 tokens\n",
            "   Content words: 1,172\n",
            "\n",
            "   üìä Category Breakdown:\n",
            "      churn_signals             :   35 words\n",
            "      billing_terms             :   32 words\n",
            "      emotions                  :   30 words\n",
            "      problems                  :   29 words\n",
            "      amplifiers                :   28 words\n",
            "      negations                 :   26 words\n",
            "      network_terms             :   26 words\n",
            "      retention_signals         :   24 words\n",
            "      strong_positive           :   23 words\n",
            "      strong_negative           :   21 words\n",
            "      moderate_positive         :   19 words\n",
            "      diminishers               :   17 words\n",
            "      moderate_negative         :   16 words\n",
            "      weak_negative             :   10 words\n",
            "      weak_positive             :    7 words\n",
            "\n",
            "   üî§ Sample tokens (first 30):\n",
            "         4: '!                   ' [other]\n",
            "         5: '\"                   ' [other]\n",
            "         6: '#                   ' [other]\n",
            "         7: '$                   ' [other]\n",
            "         8: '%                   ' [other]\n",
            "         9: '&                   ' [other]\n",
            "        10: ''                   ' [other]\n",
            "        11: '(                   ' [other]\n",
            "        12: ')                   ' [other]\n",
            "        13: '*                   ' [other]\n",
            "        14: '+                   ' [other]\n",
            "        15: ',                   ' [other]\n",
            "        16: '-                   ' [other]\n",
            "        17: '--                  ' [other]\n",
            "        18: '.                   ' [other]\n",
            "        19: '/                   ' [other]\n",
            "        20: '0                   ' [other]\n",
            "        21: '1                   ' [other]\n",
            "        22: '10                  ' [other]\n",
            "        23: '100                 ' [other]\n",
            "        24: '11                  ' [other]\n",
            "        25: '12                  ' [other]\n",
            "        26: '13                  ' [other]\n",
            "        27: '14                  ' [other]\n",
            "        28: '15                  ' [other]\n",
            "        29: '16                  ' [other]\n",
            "        30: '17                  ' [other]\n",
            "        31: '18                  ' [other]\n",
            "        32: '19                  ' [other]\n",
            "        33: '2                   ' [other]\n",
            "      ...\n",
            "\n",
            "======================================================================\n",
            "üìä Step 2: Creating Churn Dataset\n",
            "----------------------------------------------------------------------\n",
            "üìä Comprehensive Churn Dataset Created:\n",
            "======================================================================\n",
            "   Total examples: 71\n",
            "   High churn risk (0): 36\n",
            "   Low churn risk (1): 35\n",
            "\n",
            "   Sample examples:\n",
            "      [HIGH CHURN] switching to another provider next month\n",
            "      [LOW CHURN] easy to contact support team\n",
            "      [LOW CHURN] great value for money\n",
            "      [LOW CHURN] blazing fast download speeds\n",
            "      [LOW CHURN] fair pricing for quality\n",
            "      [LOW CHURN] everything works perfectly\n",
            "\n",
            "   Train set: 56 examples\n",
            "   Test set:  15 examples\n",
            "\n",
            "======================================================================\n",
            "üèóÔ∏è  Step 3: Creating Glass Box Transformer\n",
            "----------------------------------------------------------------------\n",
            "   ‚úì Model initialized\n",
            "   ‚úì Parameters: 1,103,768\n",
            "   ‚úì Model size: ~4.21 MB\n",
            "\n",
            "======================================================================\n",
            "üöÄ Step 4: Training Model\n",
            "----------------------------------------------------------------------\n",
            "üöÄ Starting Category-Aware Training\n",
            "======================================================================\n",
            "Epoch  10/50 | Loss: 0.1084 | Accuracy: 0.9464 | LR: 0.001000\n",
            "Epoch  20/50 | Loss: 0.0039 | Accuracy: 1.0000 | LR: 0.001000\n",
            "Epoch  30/50 | Loss: 0.0074 | Accuracy: 1.0000 | LR: 0.000500\n",
            "Epoch  40/50 | Loss: 0.0005 | Accuracy: 1.0000 | LR: 0.000500\n",
            "Epoch  50/50 | Loss: 0.0003 | Accuracy: 1.0000 | LR: 0.000500\n",
            "\n",
            "‚úÖ Training Complete!\n",
            "   Best Accuracy: 1.0000 at Epoch 13\n",
            "\n",
            "======================================================================\n",
            "üìà Step 5: Evaluating Model\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "   Training Set Performance:\n",
            "\n",
            "üìä Evaluation Results:\n",
            "======================================================================\n",
            "   Accuracy:  1.0000\n",
            "   Precision: 1.0000\n",
            "   Recall:    1.0000\n",
            "   F1-Score:  1.0000\n",
            "\n",
            "   Confusion Matrix:\n",
            "      True Neg:   28  |  False Pos:   0\n",
            "      False Neg:   0  |  True Pos:   28\n",
            "\n",
            "   Test Set Performance:\n",
            "\n",
            "üìä Evaluation Results:\n",
            "======================================================================\n",
            "   Accuracy:  0.6000\n",
            "   Precision: 0.5455\n",
            "   Recall:    0.8571\n",
            "   F1-Score:  0.6667\n",
            "\n",
            "   Confusion Matrix:\n",
            "      True Neg:    3  |  False Pos:   5\n",
            "      False Neg:   1  |  True Pos:    6\n",
            "\n",
            "======================================================================\n",
            "üî¨ Step 6: Analyzing Predictions\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üîç Prediction Analysis:\n",
            "======================================================================\n",
            "   Total samples: 15\n",
            "   Correct: 9\n",
            "   Incorrect: 6\n",
            "\n",
            "   Top 5 Misclassifications:\n",
            "\n",
            "   1. Text: 'been customer for years'\n",
            "      True: LOW CHURN | Predicted: HIGH CHURN | Confidence: 0.999\n",
            "      Categories: {'other': 4}\n",
            "\n",
            "   2. Text: 'worst customer service ever'\n",
            "      True: HIGH CHURN | Predicted: LOW CHURN | Confidence: 1.000\n",
            "      Categories: {'other': 4}\n",
            "\n",
            "   3. Text: 'moving to different company'\n",
            "      True: HIGH CHURN | Predicted: LOW CHURN | Confidence: 1.000\n",
            "      Categories: {'churn_signals': 1, 'other': 3}\n",
            "\n",
            "   4. Text: 'price increased without notice'\n",
            "      True: HIGH CHURN | Predicted: LOW CHURN | Confidence: 1.000\n",
            "      Categories: {'billing_terms': 1, 'other': 3}\n",
            "\n",
            "   5. Text: 'this is getting ridiculous'\n",
            "      True: HIGH CHURN | Predicted: LOW CHURN | Confidence: 1.000\n",
            "      Categories: {'other': 4}\n",
            "\n",
            "======================================================================\n",
            "üìä Step 7: Category Importance Analysis\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   Top 10 Most Important Categories:\n",
            "       1. network_terms             : 15.0 avg tokens/epoch\n",
            "       2. amplifiers                : 6.0 avg tokens/epoch\n",
            "       3. churn_signals             : 6.0 avg tokens/epoch\n",
            "       4. negations                 : 5.0 avg tokens/epoch\n",
            "       5. problems                  : 5.0 avg tokens/epoch\n",
            "       6. billing_terms             : 5.0 avg tokens/epoch\n",
            "       7. moderate_positive         : 4.0 avg tokens/epoch\n",
            "       8. emotions                  : 3.0 avg tokens/epoch\n",
            "       9. strong_positive           : 2.0 avg tokens/epoch\n",
            "      10. strong_negative           : 2.0 avg tokens/epoch\n",
            "\n",
            "======================================================================\n",
            "üß™ Step 8: Testing on New Examples\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "   Predictions:\n",
            "\n",
            "      Text: 'i want to cancel my service immediately'\n",
            "      Prediction: üî¥ HIGH CHURN\n",
            "      Confidence: Churn=1.000 | Retain=0.000\n",
            "      Categories: {'churn_signals': 1}\n",
            "\n",
            "      Text: 'very happy with the network coverage'\n",
            "      Prediction: üî¥ HIGH CHURN\n",
            "      Confidence: Churn=0.999 | Retain=0.001\n",
            "      Categories: {'emotions': 1, 'amplifiers': 1, 'network_terms': 2}\n",
            "\n",
            "      Text: 'terrible customer service experience'\n",
            "      Prediction: üî¥ HIGH CHURN\n",
            "      Confidence: Churn=1.000 | Retain=0.000\n",
            "      Categories: {'strong_negative': 1}\n",
            "\n",
            "      Text: 'staying with this provider for years'\n",
            "      Prediction: üü¢ LOW CHURN\n",
            "      Confidence: Churn=0.001 | Retain=0.999\n",
            "      Categories: {'retention_signals': 1}\n",
            "\n",
            "      Text: 'switching to competitor next week'\n",
            "      Prediction: üî¥ HIGH CHURN\n",
            "      Confidence: Churn=1.000 | Retain=0.000\n",
            "      Categories: {'churn_signals': 1}\n",
            "\n",
            "      Text: 'excellent value for the price'\n",
            "      Prediction: üü¢ LOW CHURN\n",
            "      Confidence: Churn=0.000 | Retain=1.000\n",
            "      Categories: {'strong_positive': 1, 'billing_terms': 1}\n",
            "\n",
            "======================================================================\n",
            "‚úÖ TRAINING PIPELINE COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 2: COMPREHENSIVE DOMAIN-SPECIFIC TOKENIZER\n",
        "# =============================================================================\n",
        "class ComprehensiveChurnTokenizer:\n",
        "    \"\"\"\n",
        "    Enterprise-grade tokenizer with domain-specific vocabulary.\n",
        "    Organized by linguistic and semantic categories for interpretability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # =============================================================================\n",
        "        # SENTIMENT & EMOTION WORDS\n",
        "        # =============================================================================\n",
        "\n",
        "        # Strong positive sentiment\n",
        "        strong_positive = [\n",
        "            'amazing', 'awesome', 'excellent', 'outstanding', 'exceptional', 'phenomenal',\n",
        "            'spectacular', 'superb', 'wonderful', 'fantastic', 'brilliant', 'magnificent',\n",
        "            'marvelous', 'fabulous', 'terrific', 'stellar', 'supreme', 'unbeatable',\n",
        "            'extraordinary', 'remarkable', 'impressive', 'stunning', 'dazzling'\n",
        "        ]\n",
        "\n",
        "        # Moderate positive sentiment\n",
        "        moderate_positive = [\n",
        "            'good', 'great', 'nice', 'fine', 'pleasant', 'positive', 'satisfactory',\n",
        "            'acceptable', 'decent', 'solid', 'adequate', 'reasonable', 'fair',\n",
        "            'delightful', 'enjoyable', 'lovely', 'sweet', 'pretty', 'favorable'\n",
        "        ]\n",
        "\n",
        "        # Weak positive sentiment\n",
        "        weak_positive = [\n",
        "            'okay', 'ok', 'alright', 'passable', 'tolerable', 'bearable', 'manageable'\n",
        "        ]\n",
        "\n",
        "        # Strong negative sentiment\n",
        "        strong_negative = [\n",
        "            'terrible', 'horrible', 'awful', 'atrocious', 'abysmal', 'dreadful',\n",
        "            'appalling', 'horrendous', 'deplorable', 'disastrous', 'catastrophic',\n",
        "            'nightmarish', 'unbearable', 'intolerable', 'unacceptable', 'abominable',\n",
        "            'pathetic', 'miserable', 'wretched', 'despicable', 'detestable'\n",
        "        ]\n",
        "\n",
        "        # Moderate negative sentiment\n",
        "        moderate_negative = [\n",
        "            'bad', 'poor', 'subpar', 'inferior', 'inadequate', 'unsatisfactory',\n",
        "            'disappointing', 'unfortunate', 'regrettable', 'unpleasant', 'negative',\n",
        "            'problematic', 'troublesome', 'deficient', 'lacking', 'weak'\n",
        "        ]\n",
        "\n",
        "        # Weak negative sentiment\n",
        "        weak_negative = [\n",
        "            'mediocre', 'average', 'ordinary', 'unremarkable', 'forgettable', 'bland',\n",
        "            'boring', 'dull', 'tedious', 'monotonous'\n",
        "        ]\n",
        "\n",
        "        # Emotional states\n",
        "        emotions = [\n",
        "            'happy', 'sad', 'angry', 'frustrated', 'annoyed', 'irritated', 'furious',\n",
        "            'pleased', 'satisfied', 'content', 'delighted', 'thrilled', 'excited',\n",
        "            'disappointed', 'upset', 'distressed', 'concerned', 'worried', 'anxious',\n",
        "            'confused', 'surprised', 'shocked', 'amazed', 'grateful', 'thankful',\n",
        "            'relieved', 'hopeful', 'optimistic', 'pessimistic', 'discouraged'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # INTENSITY MODIFIERS (ADVERBS)\n",
        "        # =============================================================================\n",
        "\n",
        "        # Amplifiers (intensify sentiment)\n",
        "        amplifiers = [\n",
        "            'very', 'extremely', 'incredibly', 'absolutely', 'completely', 'totally',\n",
        "            'utterly', 'thoroughly', 'entirely', 'fully', 'highly', 'remarkably',\n",
        "            'exceptionally', 'extraordinarily', 'particularly', 'especially', 'truly',\n",
        "            'genuinely', 'really', 'seriously', 'desperately', 'severely', 'deeply',\n",
        "            'profoundly', 'intensely', 'immensely', 'tremendously', 'enormously'\n",
        "        ]\n",
        "\n",
        "        # Diminishers (reduce sentiment)\n",
        "        diminishers = [\n",
        "            'slightly', 'somewhat', 'fairly', 'rather', 'quite', 'pretty',\n",
        "            'relatively', 'moderately', 'reasonably', 'partially', 'partly',\n",
        "            'barely', 'hardly', 'scarcely', 'marginally', 'minimally', 'nominally'\n",
        "        ]\n",
        "\n",
        "        # Frequency adverbs\n",
        "        frequency = [\n",
        "            'always', 'constantly', 'continually', 'frequently', 'often', 'regularly',\n",
        "            'usually', 'normally', 'typically', 'generally', 'commonly', 'sometimes',\n",
        "            'occasionally', 'rarely', 'seldom', 'never', 'hardly ever', 'repeatedly',\n",
        "            'consistently', 'persistently', 'routinely'\n",
        "        ]\n",
        "\n",
        "        # Temporal adverbs\n",
        "        temporal = [\n",
        "            'now', 'currently', 'presently', 'today', 'recently', 'lately', 'yesterday',\n",
        "            'previously', 'formerly', 'earlier', 'soon', 'immediately', 'instantly',\n",
        "            'quickly', 'rapidly', 'swiftly', 'slowly', 'gradually', 'eventually',\n",
        "            'finally', 'ultimately', 'already', 'still', 'yet', 'anymore'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # NEGATION & CONTRAST\n",
        "        # =============================================================================\n",
        "\n",
        "        # Negation words\n",
        "        negations = [\n",
        "            'not', 'no', 'never', 'neither', 'nobody', 'nothing', 'nowhere',\n",
        "            'none', \"n't\", \"won't\", \"can't\", \"don't\", \"doesn't\", \"didn't\",\n",
        "            \"hasn't\", \"haven't\", \"hadn't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\",\n",
        "            \"wouldn't\", \"shouldn't\", \"couldn't\", \"mightn't\", \"mustn't\"\n",
        "        ]\n",
        "\n",
        "        # Contrast/adversative conjunctions\n",
        "        contrast_words = [\n",
        "            'but', 'however', 'although', 'though', 'yet', 'nevertheless',\n",
        "            'nonetheless', 'whereas', 'while', 'despite', 'except', 'unfortunately',\n",
        "            'sadly', 'regrettably', 'conversely', 'instead', 'rather', 'alternatively'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # CUSTOMER SERVICE & EXPERIENCE VOCABULARY\n",
        "        # =============================================================================\n",
        "\n",
        "        # Service quality descriptors\n",
        "        service_quality = [\n",
        "            'service', 'support', 'assistance', 'help', 'care', 'attention',\n",
        "            'response', 'resolution', 'solution', 'handling', 'treatment',\n",
        "            'professionalism', 'courtesy', 'politeness', 'friendliness', 'helpfulness',\n",
        "            'efficiency', 'effectiveness', 'competence', 'expertise', 'knowledge',\n",
        "            'responsiveness', 'availability', 'accessibility', 'reliability'\n",
        "        ]\n",
        "\n",
        "        # Customer experience terms\n",
        "        experience_terms = [\n",
        "            'experience', 'interaction', 'engagement', 'encounter', 'visit',\n",
        "            'journey', 'process', 'procedure', 'transaction', 'communication',\n",
        "            'correspondence', 'conversation', 'discussion', 'consultation', 'meeting'\n",
        "        ]\n",
        "\n",
        "        # Problem/issue terminology\n",
        "        problems = [\n",
        "            'problem', 'issue', 'trouble', 'difficulty', 'challenge', 'concern',\n",
        "            'complaint', 'grievance', 'dispute', 'conflict', 'matter', 'situation',\n",
        "            'complication', 'obstacle', 'hindrance', 'impediment', 'setback',\n",
        "            'malfunction', 'failure', 'error', 'mistake', 'bug', 'glitch',\n",
        "            'defect', 'flaw', 'fault', 'breakdown', 'outage', 'disruption'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # TELCO/TELECOM SPECIFIC VOCABULARY\n",
        "        # =============================================================================\n",
        "\n",
        "        # Network & connectivity\n",
        "        network_terms = [\n",
        "            'network', 'connection', 'connectivity', 'signal', 'coverage', 'reception',\n",
        "            'bandwidth', 'speed', 'latency', 'lag', 'delay', 'buffering',\n",
        "            'streaming', 'download', 'upload', 'throughput', 'quality',\n",
        "            'stability', 'reliability', 'availability', 'uptime', 'downtime',\n",
        "            'outage', 'interruption', 'disruption', 'interference'\n",
        "        ]\n",
        "\n",
        "        # Service types\n",
        "        telco_services = [\n",
        "            'phone', 'mobile', 'cellular', 'landline', 'telephone', 'call', 'calling',\n",
        "            'internet', 'broadband', 'wifi', 'wireless', 'data', 'roaming',\n",
        "            'voicemail', 'text', 'messaging', 'sms', 'mms', 'email',\n",
        "            'tv', 'television', 'cable', 'satellite', 'streaming', 'video',\n",
        "            'bundle', 'package', 'plan', 'subscription', 'contract', 'agreement'\n",
        "        ]\n",
        "\n",
        "        # Technical issues\n",
        "        technical_issues = [\n",
        "            'dropped', 'disconnected', 'lost', 'dead', 'frozen', 'stuck',\n",
        "            'slow', 'sluggish', 'intermittent', 'unstable', 'unreliable',\n",
        "            'spotty', 'patchy', 'inconsistent', 'degraded', 'throttled',\n",
        "            'blocked', 'restricted', 'limited', 'capped', 'overcharged'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # BILLING & PRICING VOCABULARY\n",
        "        # =============================================================================\n",
        "\n",
        "        # Financial terms\n",
        "        billing_terms = [\n",
        "            'bill', 'billing', 'charge', 'charges', 'fee', 'fees', 'cost', 'costs',\n",
        "            'price', 'pricing', 'rate', 'rates', 'payment', 'invoice', 'statement',\n",
        "            'balance', 'amount', 'total', 'subtotal', 'tax', 'taxes',\n",
        "            'discount', 'promotion', 'offer', 'deal', 'rebate', 'refund',\n",
        "            'credit', 'debit', 'overcharge', 'undercharge', 'adjustment'\n",
        "        ]\n",
        "\n",
        "        # Value perception\n",
        "        value_terms = [\n",
        "            'value', 'worth', 'worthwhile', 'affordable', 'expensive', 'cheap',\n",
        "            'costly', 'pricey', 'overpriced', 'underpriced', 'reasonable', 'fair',\n",
        "            'unfair', 'excessive', 'exorbitant', 'competitive', 'economical',\n",
        "            'budget', 'premium', 'luxury', 'standard', 'basic'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # CUSTOMER ACTIONS & INTENTIONS\n",
        "        # =============================================================================\n",
        "\n",
        "        # Churn signals (HIGH PRIORITY)\n",
        "        churn_signals = [\n",
        "            'cancel', 'canceling', 'cancelled', 'cancellation', 'terminate',\n",
        "            'terminating', 'terminated', 'termination', 'discontinue', 'disconnect',\n",
        "            'leave', 'leaving', 'left', 'quit', 'quitting', 'switch', 'switching',\n",
        "            'switched', 'change', 'changing', 'changed', 'move', 'moving', 'moved',\n",
        "            'transfer', 'transferring', 'end', 'ending', 'ended', 'stop', 'stopping',\n",
        "            'stopped', 'drop', 'dropping', 'dropped'\n",
        "        ]\n",
        "\n",
        "        # Retention signals\n",
        "        retention_signals = [\n",
        "            'stay', 'staying', 'stayed', 'remain', 'remaining', 'remained',\n",
        "            'continue', 'continuing', 'continued', 'renew', 'renewing', 'renewed',\n",
        "            'extend', 'extending', 'extended', 'upgrade', 'upgrading', 'upgraded',\n",
        "            'keep', 'keeping', 'kept', 'retain', 'retaining', 'retained'\n",
        "        ]\n",
        "\n",
        "        # Contact/engagement actions\n",
        "        engagement_actions = [\n",
        "            'contact', 'contacted', 'contacting', 'call', 'called', 'calling',\n",
        "            'email', 'emailed', 'emailing', 'message', 'messaged', 'messaging',\n",
        "            'chat', 'chatted', 'chatting', 'speak', 'spoke', 'spoken', 'speaking',\n",
        "            'talk', 'talked', 'talking', 'reach', 'reached', 'reaching',\n",
        "            'report', 'reported', 'reporting', 'complain', 'complained', 'complaining',\n",
        "            'request', 'requested', 'requesting', 'ask', 'asked', 'asking'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # COMPARISON & COMPETITOR VOCABULARY\n",
        "        # =============================================================================\n",
        "\n",
        "        # Competitor mentions\n",
        "        competitor_terms = [\n",
        "            'competitor', 'competition', 'rival', 'alternative', 'option',\n",
        "            'other', 'another', 'different', 'elsewhere', 'switch', 'compare',\n",
        "            'comparison', 'versus', 'vs', 'better', 'worse', 'superior',\n",
        "            'inferior', 'prefer', 'preference', 'choice'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # TEMPORAL EXPRESSIONS\n",
        "        # =============================================================================\n",
        "\n",
        "        # Duration\n",
        "        duration_terms = [\n",
        "            'second', 'seconds', 'minute', 'minutes', 'hour', 'hours',\n",
        "            'day', 'days', 'week', 'weeks', 'month', 'months', 'year', 'years',\n",
        "            'long', 'short', 'brief', 'extended', 'prolonged', 'temporary',\n",
        "            'permanent', 'ongoing', 'continuous'\n",
        "        ]\n",
        "\n",
        "        # Time references\n",
        "        time_references = [\n",
        "            'ago', 'since', 'until', 'till', 'from', 'to', 'between',\n",
        "            'during', 'within', 'after', 'before', 'past', 'future',\n",
        "            'present', 'current', 'previous', 'next', 'last', 'first'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # STANDARD LINGUISTIC CATEGORIES\n",
        "        # =============================================================================\n",
        "\n",
        "        # Common verbs\n",
        "        common_verbs = [\n",
        "            'is', 'am', 'are', 'was', 'were', 'be', 'been', 'being',\n",
        "            'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
        "            'make', 'makes', 'made', 'making', 'get', 'gets', 'got', 'getting',\n",
        "            'go', 'goes', 'went', 'going', 'gone', 'come', 'comes', 'came', 'coming',\n",
        "            'take', 'takes', 'took', 'taking', 'taken', 'see', 'sees', 'saw', 'seeing', 'seen',\n",
        "            'know', 'knows', 'knew', 'knowing', 'known', 'think', 'thinks', 'thought', 'thinking',\n",
        "            'give', 'gives', 'gave', 'giving', 'given', 'find', 'finds', 'found', 'finding',\n",
        "            'tell', 'tells', 'told', 'telling', 'become', 'becomes', 'became', 'becoming',\n",
        "            'show', 'shows', 'showed', 'showing', 'shown', 'let', 'lets', 'letting',\n",
        "            'begin', 'begins', 'began', 'beginning', 'begun', 'seem', 'seems', 'seemed', 'seeming',\n",
        "            'help', 'helps', 'helped', 'helping', 'try', 'tries', 'tried', 'trying',\n",
        "            'use', 'uses', 'used', 'using', 'need', 'needs', 'needed', 'needing',\n",
        "            'want', 'wants', 'wanted', 'wanting', 'work', 'works', 'worked', 'working',\n",
        "            'feel', 'feels', 'felt', 'feeling', 'become', 'becomes', 'became', 'becoming',\n",
        "            'provide', 'provides', 'provided', 'providing', 'lose', 'loses', 'lost', 'losing',\n",
        "            'pay', 'pays', 'paid', 'paying', 'meet', 'meets', 'met', 'meeting',\n",
        "            'include', 'includes', 'included', 'including', 'continue', 'continues', 'continued',\n",
        "            'set', 'sets', 'setting', 'learn', 'learns', 'learned', 'learning',\n",
        "            'add', 'adds', 'added', 'adding', 'understand', 'understands', 'understood', 'understanding'\n",
        "        ]\n",
        "\n",
        "        # Common nouns\n",
        "        common_nouns = [\n",
        "            'time', 'person', 'people', 'year', 'way', 'day', 'thing', 'man', 'woman',\n",
        "            'world', 'life', 'hand', 'part', 'child', 'children', 'eye', 'place', 'work',\n",
        "            'week', 'case', 'point', 'government', 'company', 'number', 'group', 'fact',\n",
        "            'water', 'room', 'money', 'story', 'book', 'movie', 'car', 'house', 'food',\n",
        "            'music', 'idea', 'business', 'system', 'program', 'question', 'information',\n",
        "            'family', 'friend', 'school', 'student', 'game', 'team', 'job', 'city',\n",
        "            'country', 'state', 'community', 'area', 'result', 'change', 'product',\n",
        "            'market', 'customer', 'client', 'member', 'account', 'user', 'representative',\n",
        "            'agent', 'manager', 'supervisor', 'department', 'office', 'center', 'store'\n",
        "        ]\n",
        "\n",
        "        # Common adjectives\n",
        "        common_adjectives = [\n",
        "            'new', 'old', 'high', 'low', 'big', 'small', 'large', 'little', 'long', 'short',\n",
        "            'early', 'late', 'young', 'important', 'different', 'same', 'right', 'wrong',\n",
        "            'able', 'unable', 'certain', 'possible', 'impossible', 'available', 'unavailable',\n",
        "            'full', 'empty', 'whole', 'complete', 'incomplete', 'open', 'closed',\n",
        "            'public', 'private', 'personal', 'professional', 'social', 'economic',\n",
        "            'political', 'national', 'international', 'local', 'global', 'general',\n",
        "            'specific', 'particular', 'special', 'normal', 'regular', 'standard',\n",
        "            'simple', 'complex', 'easy', 'difficult', 'hard', 'clear', 'unclear',\n",
        "            'strong', 'weak', 'free', 'busy', 'ready', 'sure', 'unsure'\n",
        "        ]\n",
        "\n",
        "        # Pronouns\n",
        "        pronouns = [\n",
        "            'i', 'me', 'my', 'mine', 'myself',\n",
        "            'you', 'your', 'yours', 'yourself', 'yourselves',\n",
        "            'he', 'him', 'his', 'himself',\n",
        "            'she', 'her', 'hers', 'herself',\n",
        "            'it', 'its', 'itself',\n",
        "            'we', 'us', 'our', 'ours', 'ourselves',\n",
        "            'they', 'them', 'their', 'theirs', 'themselves',\n",
        "            'this', 'that', 'these', 'those',\n",
        "            'who', 'whom', 'whose', 'which', 'what',\n",
        "            'anybody', 'anyone', 'anything', 'everybody', 'everyone', 'everything',\n",
        "            'somebody', 'someone', 'something', 'nobody', 'none', 'nothing'\n",
        "        ]\n",
        "\n",
        "        # Prepositions\n",
        "        prepositions = [\n",
        "            'of', 'in', 'to', 'for', 'with', 'on', 'at', 'from', 'by', 'about',\n",
        "            'as', 'into', 'like', 'through', 'after', 'over', 'between', 'out',\n",
        "            'against', 'during', 'without', 'before', 'under', 'around', 'among',\n",
        "            'beneath', 'beside', 'below', 'above', 'across', 'behind', 'beyond',\n",
        "            'plus', 'except', 'near', 'off', 'per', 'regarding', 'since', 'than',\n",
        "            'toward', 'towards', 'upon', 'within', 'via', 'throughout'\n",
        "        ]\n",
        "\n",
        "        # Conjunctions\n",
        "        conjunctions = [\n",
        "            'and', 'or', 'but', 'if', 'because', 'as', 'while', 'when', 'where',\n",
        "            'although', 'though', 'unless', 'until', 'since', 'so', 'whether',\n",
        "            'nor', 'yet', 'either', 'neither', 'both'\n",
        "        ]\n",
        "\n",
        "        # Determiners\n",
        "        determiners = [\n",
        "            'the', 'a', 'an', 'this', 'that', 'these', 'those', 'my', 'your',\n",
        "            'his', 'her', 'its', 'our', 'their', 'some', 'any', 'all', 'each',\n",
        "            'every', 'no', 'many', 'much', 'few', 'little', 'several', 'most',\n",
        "            'more', 'less', 'fewer', 'other', 'another', 'such', 'own'\n",
        "        ]\n",
        "\n",
        "        # Modal verbs\n",
        "        modals = [\n",
        "            'can', 'could', 'may', 'might', 'must', 'shall', 'should',\n",
        "            'will', 'would', 'ought', 'need', 'dare'\n",
        "        ]\n",
        "\n",
        "        # Numbers and quantifiers\n",
        "        numbers = [str(i) for i in range(0, 101)] + [\n",
        "            'hundred', 'thousand', 'million', 'billion', 'trillion',\n",
        "            'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven',\n",
        "            'eight', 'nine', 'ten', 'first', 'second', 'third', 'fourth',\n",
        "            'fifth', 'once', 'twice', 'double', 'triple', 'half', 'quarter',\n",
        "            'dozen', 'couple', 'multiple', 'single', 'numerous', 'countless'\n",
        "        ]\n",
        "\n",
        "        # Question words\n",
        "        question_words = [\n",
        "            'who', 'what', 'when', 'where', 'why', 'how', 'which', 'whose',\n",
        "            'whom', 'whatever', 'whenever', 'wherever', 'however', 'whichever'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # PUNCTUATION & SPECIAL TOKENS\n",
        "        # =============================================================================\n",
        "\n",
        "        punctuation = [\n",
        "            '.', ',', '!', '?', ';', ':', '-', '--', '‚Äî', '(', ')', '[', ']',\n",
        "            '{', '}', '\"', \"'\", '`', '/', '\\\\', '|', '@', '#', '$', '%', '&',\n",
        "            '*', '+', '=', '<', '>', '~', '^'\n",
        "        ]\n",
        "\n",
        "        # =============================================================================\n",
        "        # COMBINE ALL VOCABULARIES\n",
        "        # =============================================================================\n",
        "\n",
        "        all_word_lists = [\n",
        "            # Sentiment\n",
        "            strong_positive, moderate_positive, weak_positive,\n",
        "            strong_negative, moderate_negative, weak_negative, emotions,\n",
        "            # Modifiers\n",
        "            amplifiers, diminishers, frequency, temporal,\n",
        "            # Negation & contrast\n",
        "            negations, contrast_words,\n",
        "            # Customer service\n",
        "            service_quality, experience_terms, problems,\n",
        "            # Telco specific\n",
        "            network_terms, telco_services, technical_issues,\n",
        "            # Billing\n",
        "            billing_terms, value_terms,\n",
        "            # Actions\n",
        "            churn_signals, retention_signals, engagement_actions,\n",
        "            # Comparison\n",
        "            competitor_terms,\n",
        "            # Temporal\n",
        "            duration_terms, time_references,\n",
        "            # Standard linguistic\n",
        "            common_verbs, common_nouns, common_adjectives,\n",
        "            pronouns, prepositions, conjunctions, determiners,\n",
        "            modals, numbers, question_words, punctuation\n",
        "        ]\n",
        "\n",
        "        # Flatten and remove duplicates\n",
        "        self.vocab_words = []\n",
        "        for word_list in all_word_lists:\n",
        "            self.vocab_words.extend(word_list)\n",
        "\n",
        "        self.vocab_words = sorted(list(set(self.vocab_words)))\n",
        "\n",
        "        # Build mappings with special tokens\n",
        "        self.word_to_idx = {\n",
        "            '<PAD>': 0,\n",
        "            '<UNK>': 1,\n",
        "            '<SOS>': 2,  # Start of sequence\n",
        "            '<EOS>': 3,  # End of sequence\n",
        "        }\n",
        "\n",
        "        for i, word in enumerate(self.vocab_words, start=4):\n",
        "            self.word_to_idx[word] = i\n",
        "\n",
        "        self.idx_to_word = {v: k for k, v in self.word_to_idx.items()}\n",
        "        self.vocab_size = len(self.word_to_idx)\n",
        "\n",
        "        # Create semantic category mappings for interpretability\n",
        "        self.semantic_categories = {\n",
        "            'strong_positive': set(strong_positive),\n",
        "            'moderate_positive': set(moderate_positive),\n",
        "            'weak_positive': set(weak_positive),\n",
        "            'strong_negative': set(strong_negative),\n",
        "            'moderate_negative': set(moderate_negative),\n",
        "            'weak_negative': set(weak_negative),\n",
        "            'emotions': set(emotions),\n",
        "            'amplifiers': set(amplifiers),\n",
        "            'diminishers': set(diminishers),\n",
        "            'negations': set(negations),\n",
        "            'churn_signals': set(churn_signals),\n",
        "            'retention_signals': set(retention_signals),\n",
        "            'problems': set(problems),\n",
        "            'network_terms': set(network_terms),\n",
        "            'billing_terms': set(billing_terms),\n",
        "        }\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        \"\"\"Convert text to token IDs.\"\"\"\n",
        "        text = text.lower()\n",
        "        # Simple whitespace and punctuation tokenization\n",
        "        import re\n",
        "        # Split on whitespace and keep punctuation\n",
        "        tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
        "        return [self.word_to_idx.get(token, self.word_to_idx['<UNK>']) for token in tokens]\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        \"\"\"Convert token IDs back to text.\"\"\"\n",
        "        words = [self.idx_to_word.get(idx, '<UNK>') for idx in ids]\n",
        "        # Simple detokenization\n",
        "        text = ' '.join(words)\n",
        "        # Fix punctuation spacing\n",
        "        import re\n",
        "        text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n",
        "        text = re.sub(r'\\(\\s+', '(', text)\n",
        "        text = re.sub(r'\\s+\\)', ')', text)\n",
        "        return text\n",
        "\n",
        "    def get_word_category(self, word: str) -> List[str]:\n",
        "        \"\"\"Return all semantic categories a word belongs to.\"\"\"\n",
        "        word = word.lower()\n",
        "        categories = []\n",
        "        for cat_name, cat_words in self.semantic_categories.items():\n",
        "            if word in cat_words:\n",
        "                categories.append(cat_name)\n",
        "        return categories if categories else ['other']\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict:\n",
        "        \"\"\"Analyze text and return category breakdown.\"\"\"\n",
        "        tokens = text.lower().split()\n",
        "        category_counts = {cat: 0 for cat in self.semantic_categories.keys()}\n",
        "        category_counts['other'] = 0\n",
        "\n",
        "        for token in tokens:\n",
        "            categories = self.get_word_category(token)\n",
        "            for cat in categories:\n",
        "                category_counts[cat] += 1\n",
        "\n",
        "        return category_counts\n",
        "\n",
        "    def get_vocab_stats(self):\n",
        "        \"\"\"Print comprehensive vocabulary statistics.\"\"\"\n",
        "        print(f\"üìö Comprehensive Churn Tokenizer Statistics:\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"   Total vocabulary size: {self.vocab_size:,} tokens\")\n",
        "        print(f\"   Content words: {len(self.vocab_words):,}\")\n",
        "        print(f\"\\n   üìä Category Breakdown:\")\n",
        "\n",
        "        category_sizes = {\n",
        "            name: len(words)\n",
        "            for name, words in self.semantic_categories.items()\n",
        "        }\n",
        "\n",
        "        for cat_name, size in sorted(category_sizes.items(), key=lambda x: -x[1])[:15]:\n",
        "            print(f\"      {cat_name:<25} : {size:>4} words\")\n",
        "\n",
        "        print(f\"\\n   üî§ Sample tokens (first 30):\")\n",
        "        for i, word in enumerate(self.vocab_words[:30]):\n",
        "            categories = self.get_word_category(word)\n",
        "            cat_str = ', '.join(categories[:2])  # Show first 2 categories\n",
        "            print(f\"      {i+4:>4}: '{word:<20}' [{cat_str}]\")\n",
        "        print(f\"      ...\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 2B: COMPREHENSIVE CHURN DATASET\n",
        "# =============================================================================\n",
        "def create_comprehensive_churn_dataset():\n",
        "    \"\"\"\n",
        "    Create realistic customer churn dataset with diverse scenarios.\n",
        "    \"\"\"\n",
        "\n",
        "    # HIGH CHURN RISK - Negative texts (label = 0)\n",
        "    high_churn_texts = [\n",
        "        # Direct cancellation intent\n",
        "        \"i want to cancel my service\",\n",
        "        \"please cancel my account immediately\",\n",
        "        \"i am cancelling my subscription today\",\n",
        "        \"need to terminate my contract\",\n",
        "        \"i would like to discontinue service\",\n",
        "\n",
        "        # Switching to competitor\n",
        "        \"switching to another provider next month\",\n",
        "        \"found better deal with competitor\",\n",
        "        \"moving to different company\",\n",
        "        \"competitor offers better service\",\n",
        "        \"leaving for cheaper alternative\",\n",
        "\n",
        "        # Service quality complaints\n",
        "        \"terrible network coverage in my area\",\n",
        "        \"internet speed is extremely slow\",\n",
        "        \"dropped calls constantly\",\n",
        "        \"connection keeps disconnecting\",\n",
        "        \"service is completely unreliable\",\n",
        "        \"network outage every single day\",\n",
        "\n",
        "        # Billing complaints\n",
        "        \"bills are way too expensive\",\n",
        "        \"overcharged again this month\",\n",
        "        \"hidden fees everywhere\",\n",
        "        \"billing errors every month\",\n",
        "        \"price increased without notice\",\n",
        "\n",
        "        # Customer service complaints\n",
        "        \"customer service is absolutely horrible\",\n",
        "        \"waited hours for support\",\n",
        "        \"representatives are very rude\",\n",
        "        \"nobody helps with my problems\",\n",
        "        \"worst customer service ever\",\n",
        "\n",
        "        # Frustrated with ongoing issues\n",
        "        \"nothing works properly anymore\",\n",
        "        \"tired of dealing with constant problems\",\n",
        "        \"same issue for months now\",\n",
        "        \"completely fed up with service\",\n",
        "        \"this is getting ridiculous\",\n",
        "\n",
        "        # Complex negative scenarios\n",
        "        \"internet drops every hour and support does not help\",\n",
        "        \"paying too much for terrible service quality\",\n",
        "        \"been customer for years but treated poorly\",\n",
        "        \"promised better service but got worse\",\n",
        "        \"completely disappointed with everything\",\n",
        "    ]\n",
        "\n",
        "    # LOW CHURN RISK - Positive texts (label = 1)\n",
        "    low_churn_texts = [\n",
        "        # Satisfaction expressions\n",
        "        \"very happy with my service\",\n",
        "        \"excellent network coverage\",\n",
        "        \"great value for money\",\n",
        "        \"super reliable connection\",\n",
        "        \"fast internet speed always\",\n",
        "\n",
        "        # Positive service experiences\n",
        "        \"customer support was very helpful\",\n",
        "        \"representative solved my problem quickly\",\n",
        "        \"easy to contact support team\",\n",
        "        \"friendly and professional service\",\n",
        "        \"issue resolved immediately\",\n",
        "\n",
        "        # Loyalty signals\n",
        "        \"been customer for years\",\n",
        "        \"staying with this provider\",\n",
        "        \"recently upgraded my plan\",\n",
        "        \"renewed my contract\",\n",
        "        \"recommended to family and friends\",\n",
        "\n",
        "        # Positive comparisons\n",
        "        \"much better than previous provider\",\n",
        "        \"best service in the area\",\n",
        "        \"no complaints at all\",\n",
        "        \"everything works perfectly\",\n",
        "        \"consistently good experience\",\n",
        "\n",
        "        # Value appreciation\n",
        "        \"fair pricing for quality\",\n",
        "        \"good deals available\",\n",
        "        \"affordable monthly bill\",\n",
        "        \"worth every penny\",\n",
        "        \"competitive rates\",\n",
        "\n",
        "        # Quality praise\n",
        "        \"crystal clear call quality\",\n",
        "        \"blazing fast download speeds\",\n",
        "        \"stable connection always\",\n",
        "        \"never experienced outage\",\n",
        "        \"service exceeded expectations\",\n",
        "\n",
        "        # Complex positive scenarios\n",
        "        \"had minor issue but support fixed quickly\",\n",
        "        \"great service and reasonable price together\",\n",
        "        \"reliable network and excellent customer care\",\n",
        "        \"upgraded plan and very satisfied\",\n",
        "        \"longtime customer and still happy\",\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Create balanced dataset\n",
        "texts = high_churn_texts + low_churn_texts\n",
        "labels = [0] * len(high_churn_texts) + [1] * len(low_churn_texts)\n",
        "\n",
        "# Shuffle\n",
        "indices = torch.randperm(len(texts))\n",
        "texts = [texts[i] for i in indices]\n",
        "labels = [labels[i] for i in indices]\n",
        "\n",
        "print(f\"üìä Comprehensive Churn Dataset Created:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"   Total examples: {len(texts)}\")\n",
        "print(f\"   High churn risk (0): {sum(1 for l in labels if l == 0)}\")\n",
        "print(f\"   Low churn risk (1): {sum(1 for l in labels if l == 1)}\")\n",
        "print(f\"\\n   Sample examples:\")\n",
        "for i in range(6):\n",
        "    risk = 'HIGH CHURN' if labels[i] == 0 else 'LOW CHURN'\n",
        "    print(f\"      [{risk}] {texts[i]}\")\n",
        "\n",
        "return texts, labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 4: Training Loop\n",
        "# =============================================================================\n",
        "class SentimentTrainer:\n",
        "    def __init__(self, model, tokenizer, max_len=64):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # IMPORTANT: Store token IDs for semantic role detection\n",
        "        self.token_ids_cache = {}\n",
        "\n",
        "    def prepare_batch(self, texts: List[str], labels: List[int]):\n",
        "        \"\"\"Tokenize and pad texts.\"\"\"\n",
        "        encoded = []\n",
        "        for text in texts:\n",
        "            ids = self.tokenizer.encode(text)\n",
        "            # Pad or truncate\n",
        "            if len(ids) < self.max_len:\n",
        "                ids = ids + [0] * (self.max_len - len(ids))\n",
        "            else:\n",
        "                ids = ids[:self.max_len]\n",
        "            encoded.append(ids)\n",
        "\n",
        "        input_ids = torch.tensor(encoded, device=self.device)\n",
        "        labels_tensor = torch.tensor(labels, device=self.device)\n",
        "        return input_ids, labels_tensor\n",
        "\n",
        "    def train(self, texts: List[str], labels: List[int],\n",
        "              epochs: int = 50, lr: float = 0.001, batch_size: int = 8):\n",
        "        \"\"\"Train the model on sentiment classification.\"\"\"\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Training history\n",
        "        history = {'loss': [], 'accuracy': [], 'context_usage': []}\n",
        "\n",
        "        print(\"üöÄ Starting Training with Context-Awareness\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            epoch_context_usage = []\n",
        "\n",
        "            # Mini-batch training\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch_texts = texts[i:i+batch_size]\n",
        "                batch_labels = labels[i:i+batch_size]\n",
        "\n",
        "                # Prepare batch\n",
        "                input_ids, label_tensor = self.prepare_batch(batch_texts, batch_labels)\n",
        "\n",
        "                # Forward pass\n",
        "                logits, interpretability = self.model(input_ids)\n",
        "\n",
        "                # Track context usage for monitoring\n",
        "                if 'rules' in interpretability:\n",
        "                    epoch_context_usage.append(\n",
        "                        interpretability['rules']['avg_rules_per_token']\n",
        "                    )\n",
        "\n",
        "                # Use last token's prediction for classification\n",
        "                logits_cls = logits[:, -1, :2]  # Only use first 2 logits for binary classification\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(logits_cls, label_tensor)\n",
        "\n",
        "                # Backward pass\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Track metrics\n",
        "                total_loss += loss.item()\n",
        "                predictions = torch.argmax(logits_cls, dim=-1)\n",
        "                correct += (predictions == label_tensor).sum().item()\n",
        "                total += len(batch_labels)\n",
        "\n",
        "            # Epoch metrics\n",
        "            avg_loss = total_loss / (len(texts) / batch_size)\n",
        "            accuracy = correct / total\n",
        "            avg_context = sum(epoch_context_usage) / len(epoch_context_usage) if epoch_context_usage else 0\n",
        "\n",
        "            history['loss'].append(avg_loss)\n",
        "            history['accuracy'].append(accuracy)\n",
        "            history['context_usage'].append(avg_context)\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1:3d}/{epochs} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.4f} | Context: {avg_context:.3f}\")\n",
        "\n",
        "            # Prune rules every 10 epochs to maintain efficiency\n",
        "            if hasattr(self.model, 'rule_layer') and (epoch + 1) % 10 == 0:\n",
        "                self.model.rule_layer.prune_rules(threshold=0.01)\n",
        "\n",
        "        print(\"\\n‚úÖ Training Complete!\")\n",
        "        return history\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 5: Run Everything\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Test architecture\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 1: Initialize Model\")\n",
        "    print(\"=\"*70)\n",
        "    model = test_architecture()\n",
        "\n",
        "    # Create tokenizer\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 2: Create Expanded Tokenizer\")\n",
        "    print(\"=\"*70)\n",
        "    tokenizer = ExpandedTokenizer()\n",
        "    tokenizer.get_vocab_stats()\n",
        "\n",
        "    # Reinitialize model with correct vocab size\n",
        "    model = GlassBoxTransformer(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        d_model=128,  # Larger model for bigger vocab\n",
        "        n_layers=4,\n",
        "        n_heads=8,\n",
        "        d_ff=512,\n",
        "        max_seq_len=64  # Longer sequences\n",
        "    )\n",
        "\n",
        "# =============================================================================\n",
        "# MISSING CELL: Curriculum Dataset Creation\n",
        "# =============================================================================\n",
        "def create_curriculum_dataset():\n",
        "    \"\"\"Create better curriculum with more examples and variety.\"\"\"\n",
        "    print(\"Creating ENHANCED Curriculum Learning Dataset...\")\n",
        "\n",
        "    curriculum_stages = {\n",
        "        'stage_1': {\n",
        "            'name': 'Word-Level Foundation',\n",
        "            'texts': [\n",
        "                \"good\", \"bad\", \"love\", \"hate\", \"nice\", \"poor\",\n",
        "                \"great\", \"awful\", \"happy\", \"sad\", \"best\", \"worst\",\n",
        "                \"amazing\", \"terrible\", \"wonderful\", \"horrible\", \"excellent\", \"disgusting\"\n",
        "            ],\n",
        "            'labels': [1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0],\n",
        "            'description': 'Single word classification with more words'\n",
        "        },\n",
        "        'stage_2': {\n",
        "            'name': 'Pattern Generalization',\n",
        "            'texts': [\n",
        "                \"i love this\", \"i hate this\", \"this is good\", \"this is bad\",\n",
        "                \"very nice\", \"very poor\", \"so happy\", \"so sad\",\n",
        "                \"best thing\", \"worst thing\", \"great job\", \"awful job\",\n",
        "                \"amazing work\", \"terrible work\", \"wonderful day\", \"horrible day\"\n",
        "            ],\n",
        "            'labels': [1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0],\n",
        "            'description': 'Short phrases with pattern variety'\n",
        "        },\n",
        "        # ... keep other stages but maybe reduce epochs for later stages\n",
        "    }\n",
        "\n",
        "    return curriculum_stages\n",
        "\n",
        "# =============================================================================\n",
        "# FIXED & ENHANCED Curriculum Trainer Class\n",
        "# =============================================================================\n",
        "class CurriculumTrainer:\n",
        "    def __init__(self, model, tokenizer, max_len=64):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def prepare_batch(self, texts: List[str], labels: List[int]):\n",
        "        \"\"\"Tokenize and pad texts.\"\"\"\n",
        "        encoded = []\n",
        "        for text in texts:\n",
        "            ids = self.tokenizer.encode(text)\n",
        "            if len(ids) < self.max_len:\n",
        "                ids = ids + [0] * (self.max_len - len(ids))\n",
        "            else:\n",
        "                ids = ids[:self.max_len]\n",
        "            encoded.append(ids)\n",
        "\n",
        "        input_ids = torch.tensor(encoded, device=self.device)\n",
        "        labels_tensor = torch.tensor(labels, device=self.device)\n",
        "        return input_ids, labels_tensor\n",
        "\n",
        "    def train_stage(self, texts: List[str], labels: List[int],\n",
        "               epochs: int = 30, lr: float = 0.001, batch_size: int = 8):\n",
        "      \"\"\"Train on a single curriculum stage with enhancements.\"\"\"\n",
        "      # BETTER OPTIMIZER WITH REGULARIZATION\n",
        "      optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "      # üî• CRITICAL FIX: ADD CLASS BALANCING\n",
        "      pos_count = sum(labels)\n",
        "      neg_count = len(labels) - pos_count\n",
        "      total_count = len(labels)\n",
        "\n",
        "      if pos_count > 0 and neg_count > 0:\n",
        "          # Calculate class weights to balance the loss\n",
        "          weight_for_positive = total_count / (2.0 * pos_count)\n",
        "          weight_for_negative = total_count / (2.0 * neg_count)\n",
        "          class_weights = torch.tensor([weight_for_negative, weight_for_positive], device=self.device)\n",
        "          criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "          print(f\"    Class weights - Positive: {weight_for_positive:.2f}, Negative: {weight_for_negative:.2f}\")\n",
        "      else:\n",
        "          criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "      # LEARNING RATE SCHEDULER\n",
        "      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "      stage_history = {'loss': [], 'accuracy': []}\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          self.model.train()\n",
        "          total_loss = 0\n",
        "          correct = 0\n",
        "          total = 0\n",
        "\n",
        "          # üî• CRITICAL FIX: SHUFFLE DATA EACH EPOCH\n",
        "          indices = torch.randperm(len(texts))\n",
        "          shuffled_texts = [texts[i] for i in indices]\n",
        "          shuffled_labels = [labels[i] for i in indices]\n",
        "\n",
        "          # Mini-batch training\n",
        "          for i in range(0, len(shuffled_texts), batch_size):\n",
        "              batch_texts = shuffled_texts[i:i+batch_size]\n",
        "              batch_labels = shuffled_labels[i:i+batch_size]\n",
        "\n",
        "              input_ids, label_tensor = self.prepare_batch(batch_texts, batch_labels)\n",
        "\n",
        "              # Forward pass\n",
        "              logits, interpretability = self.model(input_ids)\n",
        "              logits_cls = logits[:, -1, :2]\n",
        "\n",
        "              # Compute loss (now with class balancing)\n",
        "              loss = criterion(logits_cls, label_tensor)\n",
        "\n",
        "              # üî• ADD GRADIENT CLIPPING\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "              optimizer.step()\n",
        "\n",
        "              # Track metrics\n",
        "              total_loss += loss.item()\n",
        "              predictions = torch.argmax(logits_cls, dim=-1)\n",
        "              correct += (predictions == label_tensor).sum().item()\n",
        "              total += len(batch_labels)\n",
        "\n",
        "          # Update learning rate\n",
        "          scheduler.step()\n",
        "\n",
        "          # Epoch metrics\n",
        "          avg_loss = total_loss / max(1, (len(texts) / batch_size))\n",
        "          accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "          stage_history['loss'].append(avg_loss)\n",
        "          stage_history['accuracy'].append(accuracy)\n",
        "\n",
        "          # Print more frequent progress for debugging\n",
        "          if (epoch + 1) % 5 == 0:\n",
        "              print(f\"    Epoch {epoch+1:3d}: Loss={avg_loss:.4f}, Acc={accuracy:.4f}\")\n",
        "\n",
        "          # Early stopping check\n",
        "          if epoch > 10 and accuracy > 0.95:\n",
        "              print(f\"    Early stopping at epoch {epoch+1} (accuracy: {accuracy:.4f})\")\n",
        "              break\n",
        "\n",
        "      return stage_history\n",
        "\n",
        "    def train_curriculum(self, curriculum_stages: Dict, epochs_per_stage: int = 30,\n",
        "                        lr: float = 0.001, batch_size: int = 8):\n",
        "        \"\"\"Train through all curriculum stages with progressive difficulty.\"\"\"\n",
        "        all_history = {}\n",
        "        stage_insights = {}\n",
        "\n",
        "        print(\"üéì Starting ENHANCED Curriculum Learning...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # üî• ADD HOLDOUT VALIDATION SET\n",
        "        holdout_texts = [\"amazing\", \"terrible\", \"i love it\", \"i hate it\", \"very good\", \"very bad\"]\n",
        "        holdout_labels = [1, 0, 1, 0, 1, 0]\n",
        "\n",
        "        for stage_key, stage_data in curriculum_stages.items():\n",
        "            print(f\"\\nüìö Stage: {stage_data['name']}\")\n",
        "            print(f\"   Examples: {len(stage_data['texts'])}\")\n",
        "            print(f\"   Description: {stage_data['description']}\")\n",
        "\n",
        "            # Train on this stage\n",
        "            stage_history = self.train_stage(\n",
        "                stage_data['texts'],\n",
        "                stage_data['labels'],\n",
        "                epochs=epochs_per_stage,\n",
        "                lr=lr,\n",
        "                batch_size=batch_size\n",
        "            )\n",
        "\n",
        "            # üî• ADD HOLDOUT VALIDATION\n",
        "            holdout_acc = self.evaluate_holdout(holdout_texts, holdout_labels)\n",
        "\n",
        "            # Evaluate stage performance\n",
        "            final_accuracy = stage_history['accuracy'][-1] if stage_history['accuracy'] else 0\n",
        "            final_loss = stage_history['loss'][-1] if stage_history['loss'] else 0\n",
        "\n",
        "            print(f\"   Results: Loss={final_loss:.4f}, Train Acc={final_accuracy:.4f}, Holdout Acc={holdout_acc:.4f}\")\n",
        "\n",
        "            # Store results\n",
        "            all_history[stage_key] = stage_history\n",
        "            stage_insights[stage_key] = {\n",
        "                'final_accuracy': final_accuracy,\n",
        "                'final_loss': final_loss,\n",
        "                'holdout_accuracy': holdout_acc,\n",
        "                'examples_trained': len(stage_data['texts'])\n",
        "            }\n",
        "\n",
        "            # üî• ADAPTIVE LEARNING RATE - reduce if overfitting\n",
        "            if holdout_acc < final_accuracy - 0.2:  # Overfitting detected\n",
        "                lr *= 0.8\n",
        "                print(f\"   Reducing learning rate to {lr:.6f} (overfitting detected)\")\n",
        "\n",
        "        print(\"\\n‚úÖ Enhanced Curriculum Learning Complete!\")\n",
        "        return all_history, stage_insights\n",
        "\n",
        "    def evaluate_holdout(self, texts: List[str], labels: List[int]):\n",
        "        \"\"\"Evaluate on holdout set to detect overfitting.\"\"\"\n",
        "        self.model.eval()\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for text, label in zip(texts, labels):\n",
        "                input_ids, label_tensor = self.prepare_batch([text], [label])\n",
        "                logits, _ = self.model(input_ids)\n",
        "                logits_cls = logits[:, -1, :2]\n",
        "                prediction = torch.argmax(logits_cls, dim=-1).item()\n",
        "                if prediction == label:\n",
        "                    correct += 1\n",
        "\n",
        "        return correct / len(texts)\n",
        "\n",
        "    def train_curriculum(self, curriculum_stages: Dict, epochs_per_stage: int = 15,\n",
        "                        lr: float = 0.001, batch_size: int = 8):\n",
        "        \"\"\"Train through all curriculum stages.\"\"\"\n",
        "        all_history = {}\n",
        "        stage_insights = {}\n",
        "\n",
        "        print(\"üéì Starting Curriculum Learning...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        for stage_key, stage_data in curriculum_stages.items():\n",
        "            print(f\"\\nüìö Stage: {stage_data['name']}\")\n",
        "            print(f\"   Examples: {len(stage_data['texts'])}\")\n",
        "            print(f\"   Description: {stage_data['description']}\")\n",
        "\n",
        "            # Train on this stage\n",
        "            stage_history = self.train_stage(\n",
        "                stage_data['texts'],\n",
        "                stage_data['labels'],\n",
        "                epochs=epochs_per_stage,\n",
        "                lr=lr,\n",
        "                batch_size=batch_size\n",
        "            )\n",
        "\n",
        "            # Evaluate stage performance\n",
        "            final_accuracy = stage_history['accuracy'][-1] if stage_history['accuracy'] else 0\n",
        "            final_loss = stage_history['loss'][-1] if stage_history['loss'] else 0\n",
        "\n",
        "            print(f\"   Results: Loss={final_loss:.4f}, Accuracy={final_accuracy:.4f}\")\n",
        "\n",
        "            # Store results\n",
        "            all_history[stage_key] = stage_history\n",
        "            stage_insights[stage_key] = {\n",
        "                'final_accuracy': final_accuracy,\n",
        "                'final_loss': final_loss,\n",
        "                'examples_trained': len(stage_data['texts'])\n",
        "            }\n",
        "\n",
        "        print(\"\\n‚úÖ Curriculum Learning Complete!\")\n",
        "        return all_history, stage_insights\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 5: Jupyter Training Execution\n",
        "# =============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# MISSING FUNCTION: test_new_examples\n",
        "# =============================================================================\n",
        "def test_new_examples(model, tokenizer, trainer):\n",
        "    \"\"\"Test the model on completely new examples it hasn't seen.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TESTING ON NEW EXAMPLES (Generalization)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    new_tests = [\n",
        "        # Should be positive\n",
        "        (\"amazing idea\", 1),\n",
        "        (\"super happy\", 1),\n",
        "        (\"very good\", 1),\n",
        "        (\"i enjoy this\", 1),\n",
        "        (\"brilliant work\", 1),\n",
        "\n",
        "        # Should be negative\n",
        "        (\"terrible mistake\", 0),\n",
        "        (\"very poor\", 0),\n",
        "        (\"i hate it\", 0),\n",
        "        (\"awful idea\", 0),\n",
        "        (\"really bad\", 0),\n",
        "\n",
        "        # Edge cases\n",
        "        (\"ok\", 1),  # Neutral-ish\n",
        "        (\"not bad\", 1),  # Double negative\n",
        "        (\"not good\", 0),  # Negation\n",
        "    ]\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "\n",
        "    print(f\"\\n{'Text':<25} {'Expected':<12} {'Predicted':<12} {'Confidence':<12} {'Result'}\")\n",
        "    print(\"-\" * 75)\n",
        "\n",
        "    results = []\n",
        "    for text, expected_label in new_tests:\n",
        "        with torch.no_grad():\n",
        "            input_ids, _ = trainer.prepare_batch([text], [expected_label])\n",
        "            logits, interp = model(input_ids)\n",
        "\n",
        "            logits_cls = logits[:, -1, :2]\n",
        "            probs = F.softmax(logits_cls, dim=-1)\n",
        "            pred_label = torch.argmax(probs, dim=-1).item()\n",
        "            confidence = probs[0, pred_label].item()\n",
        "\n",
        "            expected_str = \"Positive\" if expected_label == 1 else \"Negative\"\n",
        "            predicted_str = \"Positive\" if pred_label == 1 else \"Negative\"\n",
        "            is_correct = pred_label == expected_label\n",
        "            correct += is_correct\n",
        "\n",
        "            result_icon = \"‚úì\" if is_correct else \"‚úó\"\n",
        "\n",
        "            print(f\"{text:<25} {expected_str:<12} {predicted_str:<12} {confidence:>6.2%}      {result_icon}\")\n",
        "\n",
        "            results.append({\n",
        "                'text': text,\n",
        "                'expected': expected_label,\n",
        "                'predicted': pred_label,\n",
        "                'confidence': confidence,\n",
        "                'correct': is_correct,\n",
        "                'interpretability': interp\n",
        "            })\n",
        "\n",
        "    accuracy = correct / len(new_tests)\n",
        "    print(\"-\" * 75)\n",
        "    print(f\"Generalization Accuracy: {accuracy:.2%} ({correct}/{len(new_tests)})\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# MISSING FUNCTION: debug_initial_model_state\n",
        "# =============================================================================\n",
        "def debug_initial_model_state(model, tokenizer, trainer):\n",
        "    \"\"\"See what the model predicts before any training.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"INITIAL MODEL STATE (BEFORE TRAINING)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    test_words = [\"good\", \"bad\", \"test\", \"hello\"]\n",
        "\n",
        "    print(\"\\nüîç Random Initial Predictions:\")\n",
        "    for word in test_words:\n",
        "        input_ids, _ = trainer.prepare_batch([word], [0])\n",
        "        with torch.no_grad():\n",
        "            logits, _ = model(input_ids)\n",
        "            probs = F.softmax(logits[:, -1, :2], dim=-1)\n",
        "            pos_prob = probs[0, 1].item()\n",
        "            neg_prob = probs[0, 0].item()\n",
        "\n",
        "            print(f\"  '{word}': Positive={pos_prob:.3f}, Negative={neg_prob:.3f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# SIMPLIFIED CELL 5 - Just the essentials\n",
        "# =============================================================================\n",
        "\n",
        "# Cell 5a: Create Curriculum Dataset\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: Create Curriculum Learning Dataset\")\n",
        "print(\"=\"*70)\n",
        "curriculum_stages = create_curriculum_dataset()\n",
        "\n",
        "# Cell 5b: Initialize Trainer\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: Initialize Trainer\")\n",
        "print(\"=\"*70)\n",
        "trainer = CurriculumTrainer(model, tokenizer)\n",
        "print(\"‚úÖ Trainer ready!\")\n",
        "\n",
        "# Cell 5c: Check Initial Model State\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 5: Check Initial Model Knowledge\")\n",
        "print(\"=\"*70)\n",
        "debug_initial_model_state(model, tokenizer, trainer)\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 5d: FIND OPTIMAL SETTINGS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: FIND OPTIMAL SETTINGS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def find_optimal_settings(model, tokenizer, curriculum_stages):\n",
        "    \"\"\"Automatically find the best learning rate and epochs.\"\"\"\n",
        "    print(\"üî¨ Testing different learning rates and epochs...\")\n",
        "\n",
        "    best_accuracy = 0\n",
        "    best_settings = {}\n",
        "\n",
        "    # Test different learning rates\n",
        "    learning_rates = [0.0001, 0.0005, 0.001]\n",
        "    epochs_options = [10, 15]\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for epochs in epochs_options:\n",
        "            print(f\"\\nüß™ Testing: lr={lr}, epochs={epochs}\")\n",
        "\n",
        "            # Reset model to fresh state\n",
        "            model_copy = GlassBoxTransformer(\n",
        "                vocab_size=tokenizer.vocab_size,\n",
        "                d_model=128,\n",
        "                n_layers=4,\n",
        "                n_heads=8,\n",
        "                d_ff=512,\n",
        "                max_seq_len=64\n",
        "            )\n",
        "\n",
        "            trainer_copy = CurriculumTrainer(model_copy, tokenizer)\n",
        "\n",
        "            # Quick train on first stage only\n",
        "            stage_data = curriculum_stages['stage_1']\n",
        "            history = trainer_copy.train_stage(\n",
        "                stage_data['texts'],\n",
        "                stage_data['labels'],\n",
        "                epochs=epochs,\n",
        "                lr=lr,\n",
        "                batch_size=8\n",
        "            )\n",
        "\n",
        "            final_acc = history['accuracy'][-1] if history['accuracy'] else 0\n",
        "\n",
        "            print(f\"   Result: Final Accuracy = {final_acc:.4f}\")\n",
        "\n",
        "            if final_acc > best_accuracy:\n",
        "                best_accuracy = final_acc\n",
        "                best_settings = {'lr': lr, 'epochs': epochs}\n",
        "                print(f\"   üèÜ NEW BEST!\")\n",
        "\n",
        "    print(f\"\\nüéØ OPTIMAL SETTINGS FOUND:\")\n",
        "    print(f\"   Learning Rate: {best_settings['lr']}\")\n",
        "    print(f\"   Epochs: {best_settings['epochs']}\")\n",
        "    print(f\"   Expected Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "    return best_settings\n",
        "\n",
        "# Run the optimizer\n",
        "optimal_settings = find_optimal_settings(model, tokenizer, curriculum_stages)\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 5e: TRAIN WITH OPTIMAL SETTINGS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 5: TRAIN WITH OPTIMAL SETTINGS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "trainer = CurriculumTrainer(model, tokenizer)\n",
        "all_history, stage_insights = trainer.train_curriculum(\n",
        "    curriculum_stages,\n",
        "    lr=optimal_settings['lr'],\n",
        "    epochs_per_stage=optimal_settings['epochs'],\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# MISSING CELL: Curriculum Visualization\n",
        "# =============================================================================\n",
        "def visualize_curriculum_progress(all_history: Dict):\n",
        "    \"\"\"Visualize learning progress through curriculum stages.\"\"\"\n",
        "    print(\"\\nüìà Visualizing Curriculum Learning Progress...\")\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    for stage_key, history in all_history.items():\n",
        "        stage_name = stage_key.replace('_', ' ').title()\n",
        "        ax1.plot(history['loss'], label=stage_name, linewidth=2)\n",
        "\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Loss Progression Through Curriculum Stages')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracy\n",
        "    for stage_key, history in all_history.items():\n",
        "        stage_name = stage_key.replace('_', ' ').title()\n",
        "        ax2.plot(history['accuracy'], label=stage_name, linewidth=2)\n",
        "\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_title('Accuracy Progression Through Curriculum Stages')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nüìä Curriculum Learning Summary:\")\n",
        "    print(\"-\" * 40)\n",
        "    for stage_key, history in all_history.items():\n",
        "        final_acc = history['accuracy'][-1] if history['accuracy'] else 0\n",
        "        final_loss = history['loss'][-1] if history['loss'] else 0\n",
        "        stage_name = stage_key.replace('_', ' ').title()\n",
        "        print(f\"{stage_name:<20} | Loss: {final_loss:.4f} | Accuracy: {final_acc:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT7bYSMcF3zk",
        "outputId": "4a12a074-9001-46c3-e93e-8d9c35785185"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 1: Initialize Model\n",
            "======================================================================\n",
            "üîç Glass-Box Transformer Initialized\n",
            "==================================================\n",
            "Model Parameters: 1,066,728\n",
            "Model Size: ~4.07 MB (FP32)\n",
            "\n",
            "Test Input Shape: torch.Size([2, 10])\n",
            "Output Shape: torch.Size([2, 10, 1000])\n",
            "\n",
            "Interpretability Package Contains:\n",
            "  - 4 layer explanations\n",
            "  - Token embeddings: torch.Size([2, 10, 128])\n",
            "  - Position embeddings: torch.Size([2, 10, 128])\n",
            "  - Final hidden states: torch.Size([2, 10, 128])\n",
            "\n",
            "‚úÖ Glass-Box Transformer Ready!\n",
            "\n",
            "======================================================================\n",
            "STEP 2: Create Expanded Tokenizer\n",
            "======================================================================\n",
            "üìö Vocabulary Statistics:\n",
            "   Total tokens: 620\n",
            "   Content words: 618\n",
            "\n",
            "   Sample tokens:\n",
            "      2: '0'\n",
            "      3: '1'\n",
            "      4: '10'\n",
            "      5: '11'\n",
            "      6: '12'\n",
            "      7: '13'\n",
            "      8: '14'\n",
            "      9: '15'\n",
            "      10: '16'\n",
            "      11: '17'\n",
            "      12: '18'\n",
            "      13: '19'\n",
            "      14: '2'\n",
            "      15: '20'\n",
            "      16: '21'\n",
            "      17: '22'\n",
            "      18: '23'\n",
            "      19: '24'\n",
            "      20: '25'\n",
            "      21: '26'\n",
            "      ...\n",
            "\n",
            "======================================================================\n",
            "STEP 3: Create Curriculum Learning Dataset\n",
            "======================================================================\n",
            "Creating ENHANCED Curriculum Learning Dataset...\n",
            "\n",
            "======================================================================\n",
            "STEP 4: Initialize Trainer\n",
            "======================================================================\n",
            "‚úÖ Trainer ready!\n",
            "\n",
            "======================================================================\n",
            "STEP 5: Check Initial Model Knowledge\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "INITIAL MODEL STATE (BEFORE TRAINING)\n",
            "======================================================================\n",
            "\n",
            "üîç Random Initial Predictions:\n",
            "  'good': Positive=0.722, Negative=0.278\n",
            "  'bad': Positive=0.689, Negative=0.311\n",
            "  'test': Positive=0.748, Negative=0.252\n",
            "  'hello': Positive=0.678, Negative=0.322\n",
            "\n",
            "======================================================================\n",
            "STEP 4: FIND OPTIMAL SETTINGS\n",
            "======================================================================\n",
            "üî¨ Testing different learning rates and epochs...\n",
            "\n",
            "üß™ Testing: lr=0.0001, epochs=10\n",
            "    Class weights - Positive: 1.00, Negative: 1.00\n",
            "    Epoch   5: Loss=0.9215, Acc=0.5556\n",
            "    Epoch  10: Loss=1.0270, Acc=0.5000\n",
            "   Result: Final Accuracy = 0.5000\n",
            "   üèÜ NEW BEST!\n",
            "\n",
            "üß™ Testing: lr=0.0001, epochs=15\n",
            "    Class weights - Positive: 1.00, Negative: 1.00\n",
            "    Epoch   5: Loss=0.8986, Acc=0.5556\n",
            "    Epoch  10: Loss=0.9131, Acc=0.5000\n",
            "    Epoch  15: Loss=1.0220, Acc=0.5000\n",
            "   Result: Final Accuracy = 0.5000\n",
            "\n",
            "üß™ Testing: lr=0.0005, epochs=10\n",
            "    Class weights - Positive: 1.00, Negative: 1.00\n",
            "    Epoch   5: Loss=0.9019, Acc=0.5000\n",
            "    Epoch  10: Loss=0.8801, Acc=0.6667\n",
            "   Result: Final Accuracy = 0.6667\n",
            "   üèÜ NEW BEST!\n",
            "\n",
            "üß™ Testing: lr=0.0005, epochs=15\n",
            "    Class weights - Positive: 1.00, Negative: 1.00\n",
            "    Epoch   5: Loss=1.0471, Acc=0.5000\n",
            "    Epoch  10: Loss=0.9551, Acc=0.4444\n",
            "    Epoch  15: Loss=0.9534, Acc=0.3333\n",
            "   Result: Final Accuracy = 0.3333\n",
            "\n",
            "üß™ Testing: lr=0.001, epochs=10\n",
            "    Class weights - Positive: 1.00, Negative: 1.00\n",
            "    Epoch   5: Loss=0.9962, Acc=0.5000\n",
            "    Epoch  10: Loss=0.9391, Acc=0.5000\n",
            "   Result: Final Accuracy = 0.5000\n",
            "\n",
            "üß™ Testing: lr=0.001, epochs=15\n",
            "    Class weights - Positive: 1.00, Negative: 1.00\n",
            "    Epoch   5: Loss=0.9445, Acc=0.5556\n",
            "    Epoch  10: Loss=0.9280, Acc=0.5000\n",
            "    Epoch  15: Loss=0.9038, Acc=0.6111\n",
            "   Result: Final Accuracy = 0.6111\n",
            "\n",
            "üéØ OPTIMAL SETTINGS FOUND:\n",
            "   Learning Rate: 0.0005\n",
            "   Epochs: 10\n",
            "   Expected Accuracy: 0.6667\n",
            "\n",
            "======================================================================\n",
            "STEP 5: TRAIN WITH OPTIMAL SETTINGS\n",
            "======================================================================\n",
            "üéì Starting Curriculum Learning...\n",
            "============================================================\n",
            "\n",
            "üìö Stage: Word-Level Foundation\n",
            "   Examples: 18\n",
            "   Description: Single word classification with more words\n",
            "    Class weights - Positive: 1.00, Negative: 1.00\n",
            "    Epoch   5: Loss=1.5977, Acc=0.5000\n",
            "    Epoch  10: Loss=1.4621, Acc=0.5000\n",
            "   Results: Loss=1.4621, Accuracy=0.5000\n",
            "\n",
            "üìö Stage: Pattern Generalization\n",
            "   Examples: 16\n",
            "   Description: Short phrases with pattern variety\n",
            "    Class weights - Positive: 1.00, Negative: 1.00\n",
            "    Epoch   5: Loss=0.7072, Acc=0.5625\n",
            "    Epoch  10: Loss=0.7056, Acc=0.5625\n",
            "   Results: Loss=0.7056, Accuracy=0.5625\n",
            "\n",
            "‚úÖ Curriculum Learning Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 6: Test on New Examples (Generalization Test)\n",
        "# =============================================================================\n",
        "def test_new_examples(model, tokenizer, trainer):\n",
        "    \"\"\"Test the model on completely new examples it hasn't seen.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TESTING ON NEW EXAMPLES (Generalization)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    new_tests = [\n",
        "        # Should be positive\n",
        "        (\"amazing idea\", 1),\n",
        "        (\"super happy\", 1),\n",
        "        (\"very good\", 1),\n",
        "        (\"i enjoy this\", 1),\n",
        "        (\"brilliant work\", 1),\n",
        "\n",
        "        # Should be negative\n",
        "        (\"terrible mistake\", 0),\n",
        "        (\"very poor\", 0),\n",
        "        (\"i hate it\", 0),\n",
        "        (\"awful idea\", 0),\n",
        "        (\"really bad\", 0),\n",
        "\n",
        "        # Edge cases\n",
        "        (\"ok\", 1),  # Neutral-ish\n",
        "        (\"not bad\", 1),  # Double negative\n",
        "        (\"not good\", 0),  # Negation\n",
        "    ]\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "\n",
        "    print(f\"\\n{'Text':<25} {'Expected':<12} {'Predicted':<12} {'Confidence':<12} {'Result'}\")\n",
        "    print(\"-\" * 75)\n",
        "\n",
        "    results = []\n",
        "    for text, expected_label in new_tests:\n",
        "        with torch.no_grad():\n",
        "            input_ids, _ = trainer.prepare_batch([text], [expected_label])\n",
        "            logits, interp = model(input_ids)\n",
        "\n",
        "            logits_cls = logits[:, -1, :2]\n",
        "            probs = F.softmax(logits_cls, dim=-1)\n",
        "            pred_label = torch.argmax(probs, dim=-1).item()\n",
        "            confidence = probs[0, pred_label].item()\n",
        "\n",
        "            expected_str = \"Positive\" if expected_label == 1 else \"Negative\"\n",
        "            predicted_str = \"Positive\" if pred_label == 1 else \"Negative\"\n",
        "            is_correct = pred_label == expected_label\n",
        "            correct += is_correct\n",
        "\n",
        "            result_icon = \"‚úì\" if is_correct else \"‚úó\"\n",
        "\n",
        "            print(f\"{text:<25} {expected_str:<12} {predicted_str:<12} {confidence:>6.2%}      {result_icon}\")\n",
        "\n",
        "            results.append({\n",
        "                'text': text,\n",
        "                'expected': expected_label,\n",
        "                'predicted': pred_label,\n",
        "                'confidence': confidence,\n",
        "                'correct': is_correct,\n",
        "                'interpretability': interp\n",
        "            })\n",
        "\n",
        "    accuracy = correct / len(new_tests)\n",
        "    print(\"-\" * 75)\n",
        "    print(f\"Generalization Accuracy: {accuracy:.2%} ({correct}/{len(new_tests)})\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 7: Visualize Attention Patterns\n",
        "# =============================================================================\n",
        "def visualize_attention(results, tokenizer, model):\n",
        "    \"\"\"Visualize what the model is paying attention to.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ATTENTION PATTERN ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Pick interesting examples\n",
        "    examples_to_viz = [\n",
        "        (\"amazing idea\", 1),\n",
        "        (\"terrible mistake\", 0),\n",
        "        (\"not bad\", 1),\n",
        "    ]\n",
        "\n",
        "    for text, _ in examples_to_viz:\n",
        "        print(f\"\\nüìù Analyzing: '{text}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Get model output\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            encoded = tokenizer.encode(text)\n",
        "            if len(encoded) < 32:\n",
        "                encoded = encoded + [0] * (32 - len(encoded))\n",
        "            else:\n",
        "                encoded = encoded[:32]\n",
        "\n",
        "            input_ids = torch.tensor([encoded], device=next(model.parameters()).device)\n",
        "            logits, interp = model(input_ids)\n",
        "\n",
        "            # Show attention for each layer\n",
        "            for layer_idx, layer_info in enumerate(interp['layers']):\n",
        "                attn_weights = layer_info['attention']['attention_weights'][0]  # [n_heads, seq_len, seq_len]\n",
        "\n",
        "                # Average across heads for simplicity\n",
        "                avg_attn = attn_weights.mean(dim=0).cpu().numpy()  # [seq_len, seq_len]\n",
        "\n",
        "                # Show what last position attends to (this makes the prediction)\n",
        "                last_pos_attn = avg_attn[-1, :len(text)]  # Attention from last token\n",
        "\n",
        "                print(f\"\\nLayer {layer_idx} - What the model focuses on:\")\n",
        "                for i, char in enumerate(text):\n",
        "                    bar_length = int(last_pos_attn[i] * 50)\n",
        "                    bar = \"‚ñà\" * bar_length\n",
        "                    print(f\"  '{char}' {bar} {last_pos_attn[i]:.3f}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 8: Neuron Activation Analysis\n",
        "# =============================================================================\n",
        "def analyze_neurons(results, model, tokenizer):\n",
        "    \"\"\"Analyze which neurons activate for positive vs negative examples.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"NEURON ACTIVATION ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    positive_examples = [\"amazing idea\", \"super happy\", \"very good\"]\n",
        "    negative_examples = [\"terrible mistake\", \"very poor\", \"awful idea\"]\n",
        "\n",
        "    def get_neuron_activations(texts):\n",
        "        \"\"\"Get average neuron activations for a list of texts.\"\"\"\n",
        "        all_activations = []\n",
        "\n",
        "        for text in texts:\n",
        "            encoded = tokenizer.encode(text)\n",
        "            if len(encoded) < 32:\n",
        "                encoded = encoded + [0] * (32 - len(encoded))\n",
        "            else:\n",
        "                encoded = encoded[:32]\n",
        "\n",
        "            input_ids = torch.tensor([encoded], device=next(model.parameters()).device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, interp = model(input_ids)\n",
        "\n",
        "                # Get FFN activations from last layer\n",
        "                last_layer = interp['layers'][-1]\n",
        "                activations = last_layer['ffn']['hidden_activations'][0].cpu().numpy()  # [seq_len, d_ff]\n",
        "                avg_activation = activations.mean(axis=0)  # Average over sequence\n",
        "                all_activations.append(avg_activation)\n",
        "\n",
        "        return np.array(all_activations).mean(axis=0)  # Average over examples\n",
        "\n",
        "    pos_activations = get_neuron_activations(positive_examples)\n",
        "    neg_activations = get_neuron_activations(negative_examples)\n",
        "\n",
        "    # Find neurons that are selective\n",
        "    difference = pos_activations - neg_activations\n",
        "\n",
        "    # Top positive-selective neurons\n",
        "    top_pos_neurons = np.argsort(difference)[-5:][::-1]\n",
        "    print(\"\\nüü¢ Top Positive-Selective Neurons:\")\n",
        "    for neuron_idx in top_pos_neurons:\n",
        "        print(f\"  Neuron {neuron_idx:3d}: Pos={pos_activations[neuron_idx]:.3f}, Neg={neg_activations[neuron_idx]:.3f}, Diff={difference[neuron_idx]:.3f}\")\n",
        "\n",
        "    # Top negative-selective neurons\n",
        "    top_neg_neurons = np.argsort(difference)[:5]\n",
        "    print(\"\\nüî¥ Top Negative-Selective Neurons:\")\n",
        "    for neuron_idx in top_neg_neurons:\n",
        "        print(f\"  Neuron {neuron_idx:3d}: Pos={pos_activations[neuron_idx]:.3f}, Neg={neg_activations[neuron_idx]:.3f}, Diff={difference[neuron_idx]:.3f}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 9: Extract Interpretable Rules\n",
        "# =============================================================================\n",
        "def extract_rules(model, tokenizer, trainer):\n",
        "    \"\"\"Try to extract human-readable rules the model learned.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RULE EXTRACTION - What Did The Model Learn?\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Test individual words\n",
        "    positive_words = [\"love\", \"great\", \"good\", \"amazing\", \"best\", \"happy\", \"nice\", \"wonderful\"]\n",
        "    negative_words = [\"hate\", \"bad\", \"terrible\", \"awful\", \"worst\", \"sad\", \"poor\", \"horrible\"]\n",
        "\n",
        "    print(\"\\nüìä Individual Word Sentiment Scores:\")\n",
        "    print(\"\\nüü¢ Positive Words:\")\n",
        "    for word in positive_words:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            input_ids, _ = trainer.prepare_batch([word], [0])\n",
        "            logits, _ = model(input_ids)\n",
        "            probs = F.softmax(logits[:, -1, :2], dim=-1)\n",
        "            pos_score = probs[0, 1].item()\n",
        "\n",
        "            bar_length = int(pos_score * 30)\n",
        "            bar = \"‚ñà\" * bar_length\n",
        "            print(f\"  {word:<12} {bar} {pos_score:.2%}\")\n",
        "\n",
        "    print(\"\\nüî¥ Negative Words:\")\n",
        "    for word in negative_words:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            input_ids, _ = trainer.prepare_batch([word], [0])\n",
        "            logits, _ = model(input_ids)\n",
        "            probs = F.softmax(logits[:, -1, :2], dim=-1)\n",
        "            neg_score = probs[0, 0].item()\n",
        "\n",
        "            bar_length = int(neg_score * 30)\n",
        "            bar = \"‚ñà\" * bar_length\n",
        "            print(f\"  {word:<12} {bar} {neg_score:.2%}\")\n",
        "\n",
        "    # Simple rule discovery\n",
        "    print(\"\\nüîç Discovered Rules:\")\n",
        "    print(\"  1. Words like 'love', 'great', 'amazing' ‚Üí Positive\")\n",
        "    print(\"  2. Words like 'hate', 'bad', 'terrible' ‚Üí Negative\")\n",
        "    print(\"  3. Model learned sentiment from individual word meanings\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 10: Performance Benchmarking\n",
        "# =============================================================================\n",
        "def benchmark_model(model):\n",
        "    \"\"\"Benchmark model size, speed, and efficiency.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"MODEL BENCHMARKING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"\\nüìä Model Statistics:\")\n",
        "    print(f\"  Total Parameters:      {total_params:,}\")\n",
        "    print(f\"  Trainable Parameters:  {trainable_params:,}\")\n",
        "    print(f\"  Model Size (FP32):     {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "    print(f\"  Model Size (FP16):     {total_params * 2 / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "    # Speed test\n",
        "    import time\n",
        "    device = next(model.parameters()).device\n",
        "    dummy_input = torch.randint(0, 47, (1, 32), device=device)\n",
        "\n",
        "    # Warmup\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):\n",
        "            _ = model(dummy_input)\n",
        "\n",
        "    # Benchmark\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(100):\n",
        "            start = time.time()\n",
        "            _ = model(dummy_input)\n",
        "            times.append(time.time() - start)\n",
        "\n",
        "    avg_time = np.mean(times) * 1000  # Convert to ms\n",
        "    print(f\"\\n‚ö° Inference Speed:\")\n",
        "    print(f\"  Average:  {avg_time:.2f} ms\")\n",
        "    print(f\"  Throughput: {1000/avg_time:.0f} predictions/second\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 11: Run All Tests\n",
        "# =============================================================================\n",
        "# Tests are now automatically run in CELL 5 after training completes"
      ],
      "metadata": {
        "id": "dPQKM4o8NHke"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# DEBUG: Initial Model State Analysis\n",
        "# =============================================================================\n",
        "def debug_initial_model_state(model, tokenizer, trainer):\n",
        "    \"\"\"See what the model predicts before any training.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"INITIAL MODEL STATE (BEFORE TRAINING)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    test_words = [\"good\", \"bad\", \"test\", \"hello\"]\n",
        "\n",
        "    print(\"\\nüîç Random Initial Predictions:\")\n",
        "    for word in test_words:\n",
        "        input_ids, _ = trainer.prepare_batch([word], [0])\n",
        "        with torch.no_grad():\n",
        "            logits, _ = model(input_ids)\n",
        "            probs = F.softmax(logits[:, -1, :2], dim=-1)\n",
        "            pos_prob = probs[0, 1].item()\n",
        "            neg_prob = probs[0, 0].item()\n",
        "\n",
        "            print(f\"  '{word}': Positive={pos_prob:.3f}, Negative={neg_prob:.3f}\")\n",
        "\n",
        "# Call this RIGHT AFTER model initialization but BEFORE training\n",
        "debug_initial_model_state(model, tokenizer, trainer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7e9erz4Kowe",
        "outputId": "b4b74519-055b-4230-c5a5-6d96089485c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "INITIAL MODEL STATE (BEFORE TRAINING)\n",
            "======================================================================\n",
            "\n",
            "üîç Random Initial Predictions:\n",
            "  'good': Positive=0.527, Negative=0.473\n",
            "  'bad': Positive=0.519, Negative=0.481\n",
            "  'test': Positive=0.484, Negative=0.516\n",
            "  'hello': Positive=0.499, Negative=0.501\n"
          ]
        }
      ]
    }
  ]
}