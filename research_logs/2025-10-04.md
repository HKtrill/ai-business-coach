# 🗓️ Research Log — 2025-10-04

## Yesterday
- Continued feature engineering with `TenureBucket`, `UsageSlope`, and derived ratios  
- Validated engineered features statistically and via Random Forest importance  
- Compared Logistic Regression vs Random Forest with top engineered features  

## Today
- Experimented with geometric and interaction-based feature engineering  
- Assessed feature frequency and contribution to False Positives/Negatives  
- Minimized 5–10% precision loss with minimal recall impact; models are now stable on unseen data  
- Tested multiple model architectures and automated threshold selection  
- Confirmed original LR → RF → RNN cascade outperforms alternative combinations  
- Updated feature set to include interaction terms like `SeniorEngagement`  

## Roadblocks
- Cross-validation initially unstable with geometric transformations  
- Some engineered interactions caused skewed distributions, requiring careful preprocessing  

## Findings
- Model generalizes well to unseen data after pruning low-signal features  
- Original LR→RF→RNN cascade remains optimal despite new transformations  
- Feature frequency assessment helps identify minimal-impact contributors  
- Interaction features like `SeniorEngagement` show potential to capture complex churn patterns  

## Tomorrow's Plan
- Apply advanced geometric/trig transformations and rate-of-change features  
- Explore polynomial features, cosine similarity, and distance metrics in feature space  
- Test more interaction terms between high-risk indicators and spending behavior  
- Continue systematic experimentation to improve precision without sacrificing recall
- Assess the data manually and try to determine what distinct feature is the most common in churn